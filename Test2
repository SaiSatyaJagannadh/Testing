# core.py - Updated with proper model name and error handling

import requests
import base64
import json
import urllib3
from urllib.parse import urlparse
from config import GITLAB_TOKEN, LLAMA_API_KEY

# Disable SSL warnings if we're going to disable verification
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class GitLabClient:
    def __init__(self, token, verify_ssl=True):
        self.token = token
        self.headers = {"PRIVATE-TOKEN": token}
        self.verify_ssl = verify_ssl
        
        # Configure session for better SSL handling
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.verify = verify_ssl
        
        # Add timeout and retry logic
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
    def get_project_info(self, repo_url):
        """Extract project info from GitLab URL"""
        try:
            parsed = urlparse(repo_url)
            gitlab_host = f"{parsed.scheme}://{parsed.netloc}"
            project_path = parsed.path.strip('/').replace('.git', '')
            
            # Get project ID - try different URL encoding approaches
            encoded_path = requests.utils.quote(project_path, safe='')
            api_url = f"{gitlab_host}/api/v4/projects/{encoded_path}"
            
            print(f"Connecting to: {api_url}")  # Debug info
            
            response = self.session.get(api_url, timeout=30)
            
            # If we get SSL error, try with SSL verification disabled
            if response.status_code == 0 or "SSL" in str(response.reason):
                print("SSL verification failed, retrying without SSL verification...")
                self.session.verify = False
                response = self.session.get(api_url, timeout=30)
            
            response.raise_for_status()
            return response.json(), gitlab_host
            
        except requests.exceptions.SSLError as e:
            print(f"SSL Error: {e}")
            print("Retrying with SSL verification disabled...")
            self.session.verify = False
            try:
                response = self.session.get(api_url, timeout=30)
                response.raise_for_status()
                return response.json(), gitlab_host
            except Exception as retry_e:
                raise Exception(f"Failed to fetch project info even without SSL verification: {str(retry_e)}")
        except requests.exceptions.ConnectionError as e:
            raise Exception(f"Connection error - check your network and GitLab URL: {str(e)}")
        except requests.exceptions.Timeout as e:
            raise Exception(f"Request timeout - GitLab server may be slow: {str(e)}")
        except requests.exceptions.HTTPError as e:
            if response.status_code == 401:
                raise Exception("Authentication failed - check your GitLab token")
            elif response.status_code == 404:
                raise Exception("Repository not found - check the URL and access permissions")
            else:
                raise Exception(f"HTTP {response.status_code}: {response.reason}")
        except Exception as e:
            raise Exception(f"Failed to fetch project info: {str(e)}")
    
    def get_repository_tree(self, gitlab_host, project_id):
        """Get repository file tree"""
        try:
            api_url = f"{gitlab_host}/api/v4/projects/{project_id}/repository/tree"
            params = {"recursive": True, "per_page": 100}
            
            all_files = []
            page = 1
            
            while True:
                params["page"] = page
                response = self.session.get(api_url, params=params, timeout=30)
                response.raise_for_status()
                
                files = response.json()
                if not files:
                    break
                    
                all_files.extend([f for f in files if f['type'] == 'blob'])
                page += 1
                
                if len(files) < 100:  # Last page
                    break
                    
                # Safety limit to prevent infinite loops
                if page > 50:
                    print("Warning: Stopped at page 50 to prevent excessive requests")
                    break
            
            return all_files
            
        except Exception as e:
            raise Exception(f"Failed to fetch repository tree: {str(e)}")
    
    def get_file_content(self, gitlab_host, project_id, file_path):
        """Get content of a specific file"""
        try:
            encoded_path = requests.utils.quote(file_path, safe='')
            api_url = f"{gitlab_host}/api/v4/projects/{project_id}/repository/files/{encoded_path}"
            
            # Try main branch first, then master
            for branch in ["main", "master", "develop"]:
                params = {"ref": branch}
                response = self.session.get(api_url, params=params, timeout=30)
                
                if response.status_code == 200:
                    file_data = response.json()
                    # Decode base64 content
                    try:
                        content = base64.b64decode(file_data['content']).decode('utf-8')
                        return content
                    except UnicodeDecodeError:
                        # Try with different encoding for binary files
                        try:
                            content = base64.b64decode(file_data['content']).decode('latin1')
                            return f"[Binary file content - {len(content)} bytes]"
                        except:
                            return f"[Unable to decode file content]"
                elif response.status_code != 404:
                    response.raise_for_status()
            
            return f"File not found in any branch: {file_path}"
            
        except Exception as e:
            return f"Error reading file {file_path}: {str(e)}"

class CodeProcessor:
    def __init__(self):
        self.supported_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.h', '.md', '.txt', '.yml', '.yaml', '.json', '.xml', '.html', '.css', '.rb', '.go', '.rs', '.php', '.ts', '.jsx', '.tsx']
    
    def filter_files(self, files):
        """Filter files by supported extensions"""
        filtered = []
        for file_info in files:
            file_path = file_info['path']
            if any(file_path.endswith(ext) for ext in self.supported_extensions):
                if not self._should_skip_file(file_path):
                    filtered.append(file_info)
        return filtered
    
    def _should_skip_file(self, file_path):
        """Skip certain files/directories"""
        skip_patterns = [
            'node_modules/', '.git/', '__pycache__/', '.venv/', 'venv/',
            '.idea/', '.vscode/', 'dist/', 'build/', 'target/', 'vendor/',
            '.next/', '.nuxt/', 'coverage/', '.nyc_output/'
        ]
        return any(pattern in file_path for pattern in skip_patterns)
    
    def chunk_content(self, files_content, max_chunk_size=4000):
        """Split content into chunks for LLaMA processing"""
        chunks = []
        current_chunk = ""
        current_files = []
        
        for file_path, content in files_content.items():
            file_section = f"\n\n=== FILE: {file_path} ===\n{content}\n"
            
            if len(current_chunk + file_section) > max_chunk_size:
                if current_chunk:
                    chunks.append({
                        'content': current_chunk,
                        'files': current_files.copy()
                    })
                current_chunk = file_section
                current_files = [file_path]
            else:
                current_chunk += file_section
                current_files.append(file_path)
        
        if current_chunk:
            chunks.append({
                'content': current_chunk,
                'files': current_files
            })
        
        return chunks

class LlamaClient:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://api.groq.com/openai/v1/chat/completions"
        
        # Test API connection and get available models
        self.available_models = self._get_available_models()
        print(f"Available models: {self.available_models}")
    
    def _get_available_models(self):
        """Get list of available models from Groq API"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            models_url = "https://api.groq.com/openai/v1/models"
            response = requests.get(models_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            models_data = response.json()
            model_ids = [model['id'] for model in models_data.get('data', [])]
            return model_ids
            
        except Exception as e:
            print(f"Error fetching available models: {e}")
            # Return common Groq model names as fallback
            return [
                "llama-3.1-70b-versatile",
                "llama-3.1-8b-instant", 
                "llama3-70b-8192",
                "llama3-8b-8192",
                "mixtral-8x7b-32768",
                "gemma-7b-it"
            ]
    
    def _select_best_model(self):
        """Select the best available model"""
        # Preferred models in order of preference
        preferred_models = [
            "llama-3.1-70b-versatile",  # Latest and most capable
            "llama3-70b-8192",          # Larger context
            "llama-3.1-8b-instant",     # Fast and efficient
            "llama3-8b-8192",           # Original fallback
            "mixtral-8x7b-32768",       # Alternative
            "gemma-7b-it"               # Last resort
        ]
        
        for model in preferred_models:
            if model in self.available_models:
                print(f"Selected model: {model}")
                return model
        
        # If none of the preferred models are available, use the first available
        if self.available_models:
            model = self.available_models[0]
            print(f"Using first available model: {model}")
            return model
        
        # Last resort fallback
        print("No models detected, using fallback: llama3-8b-8192")
        return "llama3-8b-8192"
    
    def generate_documentation(self, code_chunk, project_name=""):
        """Generate documentation for code chunk using LLaMA"""
        model = self._select_best_model()
        
        prompt = f"""You are an expert technical documentation generator. Analyze the provided code and generate comprehensive documentation.

Project: {project_name}
Files included: {', '.join(code_chunk['files'])}

Code Content:
{code_chunk['content']}

Generate detailed documentation that includes:
1. **Overview**: Brief description of what these files do
2. **Key Components**: Main classes, functions, and their purposes  
3. **Dependencies**: External libraries and frameworks used
4. **Code Structure**: How the code is organized
5. **Technical Details**: Important implementation details

Format the response in clear markdown with proper headers and code examples where relevant."""

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 2000,
            "temperature": 0.3
        }
        
        try:
            print(f"Making API request to {self.base_url} with model {model}")
            response = requests.post(self.base_url, headers=headers, json=payload, timeout=60)
            
            # Print response details for debugging
            print(f"Response status: {response.status_code}")
            if response.status_code != 200:
                print(f"Response text: {response.text}")
            
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
            
        except requests.exceptions.HTTPError as e:
            if response.status_code == 401:
                return f"Authentication Error: Invalid API key. Please check your LLAMA_API_KEY in the .env file."
            elif response.status_code == 429:
                return f"Rate Limit Error: Too many requests. Please try again later."
            elif response.status_code == 400:
                return f"Bad Request: {response.text}. The model '{model}' might not be available."
            else:
                return f"HTTP Error {response.status_code}: {response.text}"
        except requests.exceptions.ConnectionError as e:
            return f"Connection Error: Unable to connect to Groq API. Check your internet connection."
        except requests.exceptions.Timeout as e:
            return f"Timeout Error: Request took too long. Try again."
        except Exception as e:
            return f"Error generating documentation: {str(e)}"
    
    def generate_project_overview(self, project_info, all_files):
        """Generate overall project documentation"""
        model = self._select_best_model()
        
        files_list = "\n".join([f"- {f}" for f in all_files[:20]])  # Limit to first 20 files
        if len(all_files) > 20:
            files_list += f"\n... and {len(all_files) - 20} more files"
        
        prompt = f"""Generate a comprehensive project README for this GitLab repository:

Project Name: {project_info.get('name', 'Unknown')}
Description: {project_info.get('description', 'No description provided')}
Default Branch: {project_info.get('default_branch', 'main')}

Files in repository:
{files_list}

Create a professional README that includes:
1. **Project Title and Description**
2. **Architecture Overview** 
3. **Technology Stack**
4. **Installation Instructions**
5. **Usage Guide**
6. **File Structure Explanation**
7. **Contributing Guidelines**

Make it comprehensive and suitable for both developers and stakeholders."""

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 3000,
            "temperature": 0.2
        }
        
        try:
            print(f"Making project overview API request with model {model}")
            response = requests.post(self.base_url, headers=headers, json=payload, timeout=60)
            
            print(f"Project overview response status: {response.status_code}")
            if response.status_code != 200:
                print(f"Project overview response text: {response.text}")
            
            response.raise_for_status()
            
            result = response.json()
            return result["choices"][0]["message"]["content"]
            
        except Exception as e:
            return f"Error generating project overview: {str(e)}"

class DocumentationGenerator:
    def __init__(self, verify_ssl=True):
        self.gitlab_client = GitLabClient(GITLAB_TOKEN, verify_ssl=verify_ssl)
        self.code_processor = CodeProcessor()
        self.llama_client = LlamaClient(LLAMA_API_KEY)
    
    def generate_documentation(self, repo_url, status_callback=None):
        """Main function to generate complete documentation"""
        try:
            # Step 1: Get project info
            if status_callback:
                status_callback("Connecting to GitLab...")
            project_info, gitlab_host = self.gitlab_client.get_project_info(repo_url)
            project_id = project_info['id']
            
            # Step 2: Get repository files
            if status_callback:
                status_callback("Fetching repository structure...")
            all_files = self.gitlab_client.get_repository_tree(gitlab_host, project_id)
            
            # Step 3: Filter and process files
            if status_callback:
                status_callback("Processing code files...")
            filtered_files = self.code_processor.filter_files(all_files)
            
            # Step 4: Get file contents
            if status_callback:
                status_callback("Reading file contents...")
            files_content = {}
            max_files = min(15, len(filtered_files))  # Limit to 15 files for demo
            
            for i, file_info in enumerate(filtered_files[:max_files]):
                if status_callback:
                    status_callback(f"Reading file {i+1}/{max_files}: {file_info['path']}")
                content = self.gitlab_client.get_file_content(gitlab_host, project_id, file_info['path'])
                if not content.startswith("Error reading") and not content.startswith("File not found"):
                    files_content[file_info['path']] = content
            
            if not files_content:
                raise Exception("No readable files found in the repository")
            
            # Step 5: Generate project overview
            if status_callback:
                status_callback("Generating project overview...")
            project_overview = self.llama_client.generate_project_overview(
                project_info, list(files_content.keys())
            )
            
            # Step 6: Process code chunks
            if status_callback:
                status_callback("Processing code for AI analysis...")
            chunks = self.code_processor.chunk_content(files_content)
            
            # Step 7: Generate documentation for each chunk
            documentation_parts = [project_overview]
            
            for i, chunk in enumerate(chunks):
                if status_callback:
                    status_callback(f"Generating documentation for chunk {i+1}/{len(chunks)}...")
                chunk_doc = self.llama_client.generate_documentation(chunk, project_info.get('name', ''))
                documentation_parts.append(f"\n\n## Code Analysis - Part {i+1}\n\n{chunk_doc}")
            
            # Step 8: Combine all documentation
            if status_callback:
                status_callback("Finalizing documentation...")
            
            final_documentation = "\n\n".join(documentation_parts)
            
            # Add metadata footer
            final_documentation += f"\n\n---\n*Documentation generated automatically from GitLab repository: {repo_url}*"
            final_documentation += f"\n*Files processed: {len(files_content)}*"
            final_documentation += f"\n*Generated on: {project_info.get('last_activity_at', 'Unknown')}*"
            
            return final_documentation
            
        except Exception as e:
            raise Exception(f"Documentation generation failed: {str(e)}")
