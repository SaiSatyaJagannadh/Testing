# Existing dependencies
python-dotenv==1.0.0
requests==2.31.0
urllib3==2.0.4

# New dependencies for evaluation and export
scikit-learn==1.3.2
numpy==1.24.3
pandas==2.0.3
nltk==3.8.1
rouge-score==0.1.2
reportlab==4.0.7
python-docx==0.8.11
markdown==3.5.1
beautifulsoup4==4.12.2
PyPDF2==3.0.1


# Add this import to your existing imports
from model_evaluation import DocumentationEvaluator


def evaluate_and_export(self):
    """Evaluate generated documentation and export to different formats"""
    content = self.text_area.get(1.0, tk.END)
    if not content.strip():
        messagebox.showwarning("Warning", "No documentation to evaluate.")
        return
    
    try:
        evaluator = DocumentationEvaluator()
        
        # Run evaluation
        self.status_label.config(text="Status: Evaluating documentation quality...")
        evaluation_results = evaluator.evaluate_documentation(content)
        
        # Export to different formats
        self.status_label.config(text="Status: Exporting to PDF and DOCX...")
        export_results = evaluator.export_documentation(content, "generated_project")
        
        # Show results
        score = evaluation_results.get('overall_score', 0)
        grade = evaluation_results.get('grade', 'Unknown')
        
        messagebox.showinfo("Evaluation Complete", 
                          f"Documentation Quality Score: {score:.2f}\n"
                          f"Grade: {grade}\n\n"
                          f"Files exported:\n"
                          f"• PDF: {export_results.get('pdf_path', 'Failed')}\n"
                          f"• DOCX: {export_results.get('docx_path', 'Failed')}")
        
    except Exception as e:
        messagebox.showerror("Error", f"Evaluation failed: {str(e)}")


# Add this button after your existing buttons
self.evaluate_button = tk.Button(button_frame, text="Evaluate & Export", 
                               command=self.evaluate_and_export, 
                               bg="#9C27B0", fg="white", font=("Arial", 10))
self.evaluate_button.pack(side=tk.LEFT, padx=(10, 0))


import json
import re
import os
import nltk
import numpy as np
from datetime import datetime
from sklearn.metrics import f1_score, confusion_matrix, classification_report
from rouge_score import rouge_scorer
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Preformatted
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from docx import Document
from docx.shared import Inches
import markdown
from bs4 import BeautifulSoup
import PyPDF2

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class DocumentationEvaluator:
    def __init__(self):
        self.template_sections = [
            "overview", "key components", "dependencies", 
            "code structure", "technical details", "installation",
            "usage", "architecture", "api", "configuration"
        ]
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        self.reference_doc = None
        self.evaluation_criteria = None
        
        # Create necessary directories
        self._create_directories()
        
        # Load reference documentation if available
        self._load_reference_documentation()
    
    def _create_directories(self):
        """Create necessary directories for exports"""
        directories = [
            "exports/generated_docs",
            "exports/pdf_exports", 
            "exports/docx_exports",
            "exports/evaluation_reports",
            "templates"
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _load_reference_documentation(self):
        """Load reference documentation from sample.pdf"""
        sample_pdf_path = "templates/sample.pdf"
        extracted_md_path = "templates/extracted_sample.md"
        criteria_path = "templates/evaluation_criteria.json"
        
        if os.path.exists(sample_pdf_path):
            try:
                # Extract text from PDF
                self.reference_doc = self._extract_pdf_content(sample_pdf_path)
                
                # Save extracted content as markdown
                with open(extracted_md_path, 'w', encoding='utf-8') as f:
                    f.write(self.reference_doc)
                
                # Generate evaluation criteria
                self.evaluation_criteria = self._generate_evaluation_criteria(self.reference_doc)
                
                # Save criteria
                with open(criteria_path, 'w', encoding='utf-8') as f:
                    json.dump(self.evaluation_criteria, f, indent=2)
                    
                print("✓ Reference documentation loaded successfully")
                
            except Exception as e:
                print(f"Warning: Could not load reference documentation: {str(e)}")
                self._set_default_criteria()
        else:
            print("No sample.pdf found. Using default evaluation criteria.")
            self._set_default_criteria()
    
    def _extract_pdf_content(self, pdf_path):
        """Extract text content from PDF file"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            raise Exception(f"Failed to extract PDF content: {str(e)}")
    
    def _generate_evaluation_criteria(self, reference_text):
        """Generate evaluation criteria based on reference document"""
        word_count = len(reference_text.split())
        headers = len(re.findall(r'^#{1,6}\s', reference_text, re.MULTILINE))
        code_blocks = len(re.findall(r'```
        
        return {
            "required_sections": self.template_sections,
            "quality_thresholds": {
                "min_word_count": max(500, int(word_count * 0.7)),
                "max_word_count": int(word_count * 1.5),
                "min_code_examples": max(1, code_blocks // 2),
                "min_headers": max(3, headers // 2)
            },
            "content_requirements": {
                "technical_depth_score": 0.7,
                "code_coverage": 0.6,
                "completeness_threshold": 0.8
            },
            "formatting_standards": {
                "markdown_compliance": True,
                "code_block_formatting": True,
                "proper_heading_hierarchy": True
            }
        }
    
    def _set_default_criteria(self):
        """Set default evaluation criteria when no reference document is available"""
        self.evaluation_criteria = {
            "required_sections": self.template_sections,
            "quality_thresholds": {
                "min_word_count": 500,
                "max_word_count": 3000,
                "min_code_examples": 2,
                "min_headers": 4
            },
            "content_requirements": {
                "technical_depth_score": 0.7,
                "code_coverage": 0.6,
                "completeness_threshold": 0.8
            },
            "formatting_standards": {
                "markdown_compliance": True,
                "code_block_formatting": True,
                "proper_heading_hierarchy": True
            }
        }
    
    def evaluate_documentation(self, generated_doc):
        """
        Comprehensive evaluation of generated documentation
        Returns detailed evaluation metrics and scores
        """
        results = {
            "timestamp": datetime.now().isoformat(),
            "overall_score": 0.0,
            "grade": "",
            "section_compliance": {},
            "quality_metrics": {},
            "f1_score": 0.0,
            "confusion_matrix": None,
            "rouge_scores": {},
            "suggestions": []
        }
        
        try:
            # 1. Section Compliance Check
            section_scores = self._evaluate_sections(generated_doc)
            results["section_compliance"] = section_scores
            
            # 2. Content Quality Metrics
            quality_scores = self._evaluate_content_quality(generated_doc)
            results["quality_metrics"] = quality_scores
            
            # 3. F1 Score and Confusion Matrix
            if self.reference_doc:
                f1, cm = self._calculate_f1_score(generated_doc, self.reference_doc)
                results["f1_score"] = f1
                results["confusion_matrix"] = cm.tolist() if cm is not None else None
                
                # 4. ROUGE Scores
                rouge_scores = self._calculate_rouge_scores(generated_doc, self.reference_doc)
                results["rouge_scores"] = rouge_scores
            
            # 5. Calculate Overall Score
            results["overall_score"] = self._calculate_overall_score(results)
            
            # 6. Assign Grade
            results["grade"] = self._assign_grade(results["overall_score"])
            
            # 7. Generate Suggestions
            results["suggestions"] = self._generate_suggestions(results)
            
            # 8. Save Evaluation Report
            self._save_evaluation_report(results)
            
            return results
            
        except Exception as e:
            print(f"Evaluation error: {str(e)}")
            return {
                "error": str(e),
                "overall_score": 0.0,
                "grade": "F - Evaluation Failed"
            }
    
    def _evaluate_sections(self, doc_content):
        """Evaluate presence and quality of required sections"""
        section_scores = {}
        doc_lower = doc_content.lower()
        
        for section in self.template_sections:
            # Check multiple patterns for section presence
            patterns = [
                f"#{1,3}.*{section}",
                f"\\*\\*{section}\\*\\*",
                f"_{section}_",
                section
            ]
            
            found = False
            for pattern in patterns:
                if re.search(pattern, doc_lower):
                    found = True
                    break
            
            # Score based on presence and content quality
            if found:
                section_content = self._extract_section_content(doc_content, section)
                word_count = len(section_content.split())
                
                if word_count >= 50:
                    section_scores[section] = 1.0
                elif word_count >= 20:
                    section_scores[section] = 0.7
                else:
                    section_scores[section] = 0.4
            else:
                section_scores[section] = 0.0
        
        return section_scores
    
    def _extract_section_content(self, doc_content, section_name):
        """Extract content for a specific section"""
        patterns = [
            f"#{1,3}.*{section_name}.*?(?=#{1,3}|$)",
            f"\\*\\*{section_name}\\*\\*.*?(?=\\*\\*|$)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, doc_content, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(0)
        
        return ""
    
    def _evaluate_content_quality(self, doc_content):
        """Evaluate overall content quality metrics"""
        word_count = len(doc_content.split())
        code_blocks = len(re.findall(r'```', doc_content))
        headers = len(re.findall(r'^#{1,6}\s', doc_content, re.MULTILINE))
        links = len(re.findall(r'\[.*?\]\(.*?\)', doc_content))
        
        criteria = self.evaluation_criteria["quality_thresholds"]
        
        return {
            "word_count": word_count,
            "word_count_score": min(word_count / criteria["min_word_count"], 1.0),
            "code_examples": code_blocks,
            "code_score": min(code_blocks / criteria["min_code_examples"], 1.0),
            "structure_score": min(headers / criteria["min_headers"], 1.0),
            "completeness": min(word_count / criteria["min_word_count"], 1.0),
            "formatting_score": self._evaluate_formatting(doc_content)
        }
    
    def _evaluate_formatting(self, doc_content):
        """Evaluate markdown formatting quality"""
        score = 0.0
        total_checks = 5
        
        # Check for proper headers
        if re.search(r'^#{1,6}\s', doc_content, re.MULTILINE):
            score += 0.2
        
        # Check for code blocks
        if '```
            score += 0.2
        
        # Check for lists
        if re.search(r'^[-*+]\s', doc_content, re.MULTILINE):
            score += 0.2
        
        # Check for bold/italic text
        if re.search(r'\*\*.*?\*\*|\*.*?\*', doc_content):
            score += 0.2
        
        # Check for links
        if re.search(r'$$.*?$$$$.*?$$', doc_content):
            score += 0.2
        
        return score
    
    def _calculate_f1_score(self, generated_doc, reference_doc):
        """Calculate F1 score comparing generated vs reference documentation"""
        try:
            # Binary classification for each required section
            gen_sections = []
            ref_sections = []
            
            for section in self.template_sections:
                gen_has_section = 1 if re.search(section, generated_doc.lower()) else 0
                ref_has_section = 1 if re.search(section, reference_doc.lower()) else 0
                
                gen_sections.append(gen_has_section)
                ref_sections.append(ref_has_section)
            
            # Calculate F1 score
            f1 = f1_score(ref_sections, gen_sections, average='weighted', zero_division=0)
            
            # Calculate confusion matrix
            cm = confusion_matrix(ref_sections, gen_sections)
            
            return f1, cm
            
        except Exception as e:
            print(f"F1 calculation error: {str(e)}")
            return 0.0, None
    
    def _calculate_rouge_scores(self, generated_doc, reference_doc):
        """Calculate ROUGE scores for content similarity"""
        try:
            scores = self.rouge_scorer.score(reference_doc, generated_doc)
            return {
                "rouge1": scores['rouge1'].fmeasure,
                "rouge2": scores['rouge2'].fmeasure,
                "rougeL": scores['rougeL'].fmeasure
            }
        except Exception as e:
            print(f"ROUGE calculation error: {str(e)}")
            return {"rouge1": 0.0, "rouge2": 0.0, "rougeL": 0.0}
    
    def _calculate_overall_score(self, results):
        """Calculate weighted overall score"""
        try:
            section_avg = np.mean(list(results["section_compliance"].values()))
            quality_avg = np.mean([
                results["quality_metrics"].get("word_count_score", 0),
                results["quality_metrics"].get("code_score", 0),
                results["quality_metrics"].get("structure_score", 0),
                results["quality_metrics"].get("formatting_score", 0)
            ])
            
            f1_score = results.get("f1_score", 0)
            rouge_avg = np.mean(list(results.get("rouge_scores", {"rouge1": 0, "rouge2": 0, "rougeL": 0}).values()))
            
            # Weighted average
            overall = (
                section_avg * 0.3 +
                quality_avg * 0.3 +
                f1_score * 0.2 +
                rouge_avg * 0.2
            )
            
            return round(overall, 3)
            
        except Exception as e:
            print(f"Overall score calculation error: {str(e)}")
            return 0.0
    
    def _assign_grade(self, score):
        """Assign letter grade based on overall score"""
        if score >= 0.9:
            return "A - Excellent"
        elif score >= 0.8:
            return "B - Good"
        elif score >= 0.7:
            return "C - Acceptable"
        elif score >= 0.6:
            return "D - Needs Improvement"
        else:
            return "F - Poor"
    
    def _generate_suggestions(self, results):
        """Generate improvement suggestions based on evaluation results"""
        suggestions = []
        
        # Section-based suggestions
        for section, score in results["section_compliance"].items():
            if score < 0.5:
                suggestions.append(f"Add or improve the '{section}' section")
        
        # Quality-based suggestions
        quality = results["quality_metrics"]
        
        if quality.get("word_count_score", 0) < 0.7:
            suggestions.append("Expand documentation with more detailed explanations")
        
        if quality.get("code_score", 0) < 0.7:
            suggestions.append("Add more code examples and snippets")
        
        if quality.get("structure_score", 0) < 0.7:
            suggestions.append("Improve document structure with more headers and sections")
        
        if quality.get("formatting_score", 0) < 0.7:
            suggestions.append("Enhance markdown formatting (bold, italic, lists, links)")
        
        # F1 score suggestions
        if results.get("f1_score", 0) < 0.7:
            suggestions.append("Better align content with reference documentation structure")
        
        return suggestions[:5]  # Limit to top 5 suggestions
    
    def _save_evaluation_report(self, results):
        """Save evaluation report to file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = f"exports/evaluation_reports/evaluation_report_{timestamp}.json"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"✓ Evaluation report saved: {report_path}")
        except Exception as e:
            print(f"Failed to save evaluation report: {str(e)}")
    
    def export_documentation(self, doc_content, project_name="documentation"):
        """Export documentation to multiple formats"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        results = {
            "timestamp": timestamp,
            "project_name": project_name
        }
        
        try:
            # Save markdown
            md_path = f"exports/generated_docs/{project_name}_{timestamp}.md"
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(doc_content)
            results["md_path"] = md_path
            
            # Export to PDF
            pdf_path = self._export_to_pdf(doc_content, project_name, timestamp)
            results["pdf_path"] = pdf_path
            
            # Export to DOCX
            docx_path = self._export_to_docx(doc_content, project_name, timestamp)
            results["docx_path"] = docx_path
            
            return results
            
        except Exception as e:
            print(f"Export error: {str(e)}")
            results["error"] = str(e)
            return results
    
    def _export_to_pdf(self, doc_content, project_name, timestamp):
        """Export documentation to PDF format"""
        try:
            pdf_path = f"exports/pdf_exports/{project_name}_{timestamp}.pdf"
            
            # Create PDF document
            doc = SimpleDocTemplate(pdf_path, pagesize=A4)
            styles = getSampleStyleSheet()
            story = []
            
            # Custom styles
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                fontSize=18,
                spaceAfter=30,
                textColor='darkblue'
            )
            
            heading_style = ParagraphStyle(
                'CustomHeading',
                parent=styles['Heading2'],
                fontSize=14,
                spaceAfter=12,
                textColor='darkgreen'
            )
            
            # Convert markdown to HTML then to PDF
            html_content = markdown.markdown(doc_content)
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Process HTML elements
            for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'pre', 'code']):
                if element.name in ['h1']:
                    story.append(Paragraph(element.get_text(), title_style))
                elif element.name in ['h2', 'h3', 'h4']:
                    story.append(Paragraph(element.get_text(), heading_style))
                elif element.name == 'pre':
                    story.append(Preformatted(element.get_text(), styles['Code']))
                else:
                    story.append(Paragraph(element.get_text(), styles['Normal']))
                story.append(Spacer(1, 12))
            
            # Build PDF
            doc.build(story)
            print(f"✓ PDF exported: {pdf_path}")
            return pdf_path
            
        except Exception as e:
            print(f"PDF export error: {str(e)}")
            return None
    
    def _export_to_docx(self, doc_content, project_name, timestamp):
        """Export documentation to DOCX format"""
        try:
            docx_path = f"exports/docx_exports/{project_name}_{timestamp}.docx"
            
            # Create Word document
            doc = Document()
            
            # Process markdown content
            lines = doc_content.split('\n')
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Headers
                if line.startswith('# '):
                    doc.add_heading(line[2:], level=1)
                elif line.startswith('## '):
                    doc.add_heading(line[3:], level=2)
                elif line.startswith('### '):
                    doc.add_heading(line[4:], level=3)
                # Code blocks
                elif line.startswith('```'):
                    continue  # Skip code block markers
                elif line.startswith('    ') or line.startswith('\t'):
                    # Indented code
                    p = doc.add_paragraph(line)
                    p.style = 'Code'
                # Lists
                elif line.startswith('- ') or line.startswith('* '):
                    doc.add_paragraph(line[2:], style='List Bullet')
                elif re.match(r'^\d+\. ', line):
                    doc.add_paragraph(line[3:], style='List Number')
                # Regular paragraphs
                else:
                    if line:
                        doc.add_paragraph(line)
            
            # Save document
            doc.save(docx_path)
            print(f"✓ DOCX exported: {docx_path}")
            return docx_path
            
        except Exception as e:
            print(f"DOCX export error: {str(e)}")
            return None

# Example usage and testing
if __name__ == "__main__":
    # Test the evaluator
    evaluator = DocumentationEvaluator()
    
    # Sample documentation for testing
    sample_doc = """
    # Project Overview
    This is a Flask web application that provides REST API endpoints.
    
    ## Key Components
    - app.py: Main application file
    - models.py: Database models
    - routes.py: API routes
    
    ## Dependencies
    - Flask==2.0.1
    - SQLAlchemy==1.4.0
    
    ## Usage
    Run the application with: python app.py
    """
    
    # Run evaluation
    results = evaluator.evaluate_documentation(sample_doc)
    print(f"Evaluation Results: {results['overall_score']:.2f} ({results['grade']})")
    
    # Export documentation
    export_results = evaluator.export_documentation(sample_doc, "test_project")
    print(f"Export completed: {export_results}")

gitlab-ai-doc-generator/
├── .env                           # Environment variables (GitLab token, LLaMA API key)
├── .gitignore                     # Git ignore file
├── README.md                      # Project documentation
├── requirements.txt               # Python dependencies (updated with new packages)
├── main.py                        # Main GUI application (existing)
├── core.py                        # Core GitLab & LLaMA functionality (existing)
├── config.py                      # Configuration management (existing)
├── azure_storage.py               # Azure integration (existing)
├── model_evaluation.py            # NEW - Model evaluation with F1, confusion matrix, PDF/DOCX export
│
├── templates/                     # Documentation templates and references
│   ├── sample.pdf                 # Your reference documentation (you provide)
│   ├── extracted_sample.md        # Auto-generated from sample.pdf
│   └── evaluation_criteria.json   # Auto-generated evaluation standards
│
└── exports/                       # Auto-generated documentation exports
    ├── generated_docs/            # Auto-generated markdown files
    ├── pdf_exports/               # Auto-generated PDF exports
    ├── docx_exports/              # Auto-generated DOCX exports
    └── evaluation_reports/        # Auto-generated evaluation results and scores
