# Core functionality
python-dotenv==1.0.0
requests==2.31.0
urllib3==2.0.4

# Template processing
PyPDF2==3.0.1
python-docx==0.8.11
pdfplumber==0.9.0

# Export capabilities
weasyprint==60.0
reportlab==4.0.4
markdown==3.4.4

# Evaluation metrics
nltk==3.8.1
scikit-learn==1.3.0
numpy==1.24.3
pandas==2.0.3
rouge-score==0.1.2

# Token counting and text processing
tiktoken==0.5.1
transformers==4.35.0
beautifulsoup4==4.12.2

# Testing framework
pytest==7.4.3
pytest-cov==4.1.0

# Visualization for metrics
matplotlib==3.7.2
seaborn==0.12.2



# GitLab Configuration
GITLAB_TOKEN=your_gitlab_token_here

# LLaMA API Configuration
LLAMA_API_KEY=your_llama_api_key_here

# Azure Configuration (Optional)
AZURE_STORAGE_CONNECTION_STRING=your_azure_connection_string
AZURE_COSMOS_CONNECTION_STRING=your_cosmos_connection_string

# Application Settings
MAX_FILES_TO_PROCESS=25
MAX_CHUNK_SIZE=4000
MAX_TOKENS_PER_REQUEST=8000

# Quality Thresholds
TEMPLATE_COMPLIANCE_THRESHOLD=0.9
CODE_ACCURACY_THRESHOLD=0.85
CONTENT_QUALITY_THRESHOLD=0.8
FILE_COVERAGE_THRESHOLD=0.95
OVERALL_QUALITY_THRESHOLD=0.85

# Token Management
TOKEN_SAFETY_MARGIN=0.75
CONTEXT_WINDOW_SIZE=128000

# Export Settings
DEFAULT_EXPORT_FORMAT=pdf


#config.py
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# GitLab Configuration
GITLAB_TOKEN = os.getenv("GITLAB_TOKEN")
if not GITLAB_TOKEN:
    raise ValueError("GITLAB_TOKEN not found in environment variables. Please add it to your .env file.")

# LLaMA API Configuration  
LLAMA_API_KEY = os.getenv("LLAMA_API_KEY")
if not LLAMA_API_KEY:
    raise ValueError("LLAMA_API_KEY not found in environment variables. Please add it to your .env file.")

# Azure Configuration (Optional)
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING")
AZURE_COSMOS_CONNECTION_STRING = os.getenv("AZURE_COSMOS_CONNECTION_STRING")

# Application Settings
MAX_FILES_TO_PROCESS = int(os.getenv("MAX_FILES_TO_PROCESS", "25"))
MAX_CHUNK_SIZE = int(os.getenv("MAX_CHUNK_SIZE", "4000"))
MAX_TOKENS_PER_REQUEST = int(os.getenv("MAX_TOKENS_PER_REQUEST", "8000"))

# Quality Thresholds
QUALITY_THRESHOLDS = {
    'template_compliance_f1': float(os.getenv("TEMPLATE_COMPLIANCE_THRESHOLD", "0.9")),
    'code_accuracy_f1': float(os.getenv("CODE_ACCURACY_THRESHOLD", "0.85")),
    'content_quality': float(os.getenv("CONTENT_QUALITY_THRESHOLD", "0.8")),
    'file_coverage': float(os.getenv("FILE_COVERAGE_THRESHOLD", "0.95")),
    'overall_quality': float(os.getenv("OVERALL_QUALITY_THRESHOLD", "0.85"))
}

# Token Management
TOKEN_SAFETY_MARGIN = float(os.getenv("TOKEN_SAFETY_MARGIN", "0.75"))
CONTEXT_WINDOW_SIZE = int(os.getenv("CONTEXT_WINDOW_SIZE", "128000"))

# Export Settings
DEFAULT_EXPORT_FORMAT = os.getenv("DEFAULT_EXPORT_FORMAT", "pdf")
EXPORT_STYLING = {
    'pdf_style': 'professional',
    'docx_template': 'corporate',
    'include_toc': True,
    'include_metadata': True
}

# Coverage Testing Settings
COVERAGE_REQUIREMENTS = {
    'min_file_coverage': 0.95,
    'critical_files_coverage': 1.0,
    'function_coverage': 0.90,
    'class_coverage': 0.90,
    'api_coverage': 0.95
}


#core.py

import requests
import base64
import json
import urllib3
from urllib.parse import urlparse
from config import GITLAB_TOKEN, LLAMA_API_KEY, MAX_CHUNK_SIZE, MAX_TOKENS_PER_REQUEST, TOKEN_SAFETY_MARGIN
import tiktoken
import time

# Disable SSL warnings if we're going to disable verification
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class GitLabClient:
    def __init__(self, token, verify_ssl=False):
        self.token = token
        self.headers = {"PRIVATE-TOKEN": token}
        self.verify_ssl = verify_ssl
        
        # Configure session for better SSL handling
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.verify = verify_ssl
        
        # Add timeout and retry logic
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
    def get_project_info(self, repo_url):
        """Extract project info from GitLab URL"""
        try:
            parsed = urlparse(repo_url)
            gitlab_host = f"{parsed.scheme}://{parsed.netloc}"
            project_path = parsed.path.strip('/').replace('.git', '')
            
            # Get project ID - try different URL encoding approaches
            encoded_path = requests.utils.quote(project_path, safe='')
            api_url = f"{gitlab_host}/api/v4/projects/{encoded_path}"
            
            print(f"Connecting to: {api_url}")
            print(f"SSL Verification: {self.verify_ssl}")
            
            response = self.session.get(api_url, timeout=30)
            response.raise_for_status()
            return response.json(), gitlab_host
            
        except requests.exceptions.SSLError as e:
            print(f"SSL Error: {e}")
            print("Retrying with SSL verification disabled...")
            self.session.verify = False
            try:
                response = self.session.get(api_url, timeout=30)
                response.raise_for_status()
                return response.json(), gitlab_host
            except Exception as retry_e:
                raise Exception(f"Failed to fetch project info even without SSL verification: {str(retry_e)}")
        except requests.exceptions.ConnectionError as e:
            raise Exception(f"Connection error - check your network and GitLab URL: {str(e)}")
        except requests.exceptions.Timeout as e:
            raise Exception(f"Request timeout - GitLab server may be slow: {str(e)}")
        except requests.exceptions.HTTPError as e:
            if response.status_code == 401:
                raise Exception("Authentication failed - check your GitLab token")
            elif response.status_code == 404:
                raise Exception("Repository not found - check the URL and access permissions")
            else:
                raise Exception(f"HTTP {response.status_code}: {response.reason}")
        except Exception as e:
            raise Exception(f"Failed to fetch project info: {str(e)}")
    
    def get_repository_tree(self, gitlab_host, project_id):
        """Get repository file tree with comprehensive coverage"""
        try:
            api_url = f"{gitlab_host}/api/v4/projects/{project_id}/repository/tree"
            params = {"recursive": True, "per_page": 100}
            
            all_files = []
            page = 1
            
            while True:
                params["page"] = page
                response = self.session.get(api_url, params=params, timeout=30)
                response.raise_for_status()
                
                files = response.json()
                if not files:
                    break
                    
                all_files.extend([f for f in files if f['type'] == 'blob'])
                page += 1
                
                if len(files) < 100:  # Last page
                    break
                    
                # Safety limit to prevent infinite loops
                if page > 100:
                    print("Warning: Stopped at page 100 to prevent excessive requests")
                    break
            
            print(f"Total files discovered: {len(all_files)}")
            return all_files
            
        except Exception as e:
            raise Exception(f"Failed to fetch repository tree: {str(e)}")
    
    def get_file_content(self, gitlab_host, project_id, file_path):
        """Get content of a specific file with enhanced error handling"""
        try:
            encoded_path = requests.utils.quote(file_path, safe='')
            api_url = f"{gitlab_host}/api/v4/projects/{project_id}/repository/files/{encoded_path}"
            
            # Try main branch first, then master, then develop
            for branch in ["main", "master", "develop"]:
                params = {"ref": branch}
                response = self.session.get(api_url, params=params, timeout=30)
                
                if response.status_code == 200:
                    file_data = response.json()
                    try:
                        content = base64.b64decode(file_data['content']).decode('utf-8')
                        return content
                    except UnicodeDecodeError:
                        try:
                            content = base64.b64decode(file_data['content']).decode('latin1')
                            return f"[Binary file content - {len(content)} bytes]"
                        except:
                            return f"[Unable to decode file content]"
                elif response.status_code != 404:
                    response.raise_for_status()
            
            return f"File not found in any branch: {file_path}"
            
        except Exception as e:
            return f"Error reading file {file_path}: {str(e)}"

class CodeProcessor:
    def __init__(self):
        self.supported_extensions = [
            '.py', '.js', '.java', '.cpp', '.c', '.h', '.md', '.txt', 
            '.yml', '.yaml', '.json', '.xml', '.html', '.css', '.rb', 
            '.go', '.rs', '.php', '.ts', '.jsx', '.tsx', '.sh', '.sql'
        ]
        self.encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 encoding
        
    def filter_files(self, files):
        """Filter files by supported extensions with priority ranking"""
        filtered = []
        priority_files = []
        
        for file_info in files:
            file_path = file_info['path']
            if any(file_path.endswith(ext) for ext in self.supported_extensions):
                if not self._should_skip_file(file_path):
                    if self._is_critical_file(file_path):
                        priority_files.append(file_info)
                    else:
                        filtered.append(file_info)
        
        # Return priority files first, then others
        return priority_files + filtered
    
    def _should_skip_file(self, file_path):
        """Skip certain files/directories"""
        skip_patterns = [
            'node_modules/', '.git/', '__pycache__/', '.venv/', 'venv/',
            '.idea/', '.vscode/', 'dist/', 'build/', 'target/', 'vendor/',
            '.next/', '.nuxt/', 'coverage/', '.nyc_output/', 'logs/',
            '.pytest_cache/', '.tox/', 'htmlcov/'
        ]
        return any(pattern in file_path for pattern in skip_patterns)
    
    def _is_critical_file(self, file_path):
        """Identify critical files that must be documented"""
        critical_patterns = [
            'main.py', 'app.py', 'index.js', 'server.js', 'config.py',
            'settings.py', 'requirements.txt', 'package.json', 'README.md',
            'Dockerfile', 'docker-compose.yml', 'setup.py'
        ]
        return any(pattern in file_path.lower() for pattern in critical_patterns)
    
    def count_tokens(self, text):
        """Count tokens in text using tiktoken"""
        return len(self.encoding.encode(text))
    
    def chunk_content(self, files_content, max_chunk_size=MAX_CHUNK_SIZE):
        """Split content into semantic chunks with token counting"""
        chunks = []
        current_chunk = ""
        current_files = []
        current_tokens = 0
        
        for file_path, content in files_content.items():
            file_section = f"\n\n=== FILE: {file_path} ===\n{content}\n"
            file_tokens = self.count_tokens(file_section)
            
            # If single file exceeds chunk size, split it
            if file_tokens > max_chunk_size:
                if current_chunk:
                    chunks.append({
                        'content': current_chunk,
                        'files': current_files.copy(),
                        'token_count': current_tokens
                    })
                    current_chunk = ""
                    current_files = []
                    current_tokens = 0
                
                # Split large file into smaller chunks
                split_chunks = self._split_large_file(file_path, content, max_chunk_size)
                chunks.extend(split_chunks)
                continue
            
            # Check if adding this file would exceed chunk size
            if current_tokens + file_tokens > max_chunk_size and current_chunk:
                chunks.append({
                    'content': current_chunk,
                    'files': current_files.copy(),
                    'token_count': current_tokens
                })
                current_chunk = file_section
                current_files = [file_path]
                current_tokens = file_tokens
            else:
                current_chunk += file_section
                current_files.append(file_path)
                current_tokens += file_tokens
        
        if current_chunk:
            chunks.append({
                'content': current_chunk,
                'files': current_files,
                'token_count': current_tokens
            })
        
        return chunks
    
    def _split_large_file(self, file_path, content, max_chunk_size):
        """Split large files while preserving semantic boundaries"""
        chunks = []
        lines = content.split('\n')
        current_chunk = ""
        current_tokens = 0
        
        header = f"\n\n=== FILE: {file_path} (Part {{part}}) ===\n"
        part_num = 1
        
        for line in lines:
            line_with_newline = line + '\n'
            line_tokens = self.count_tokens(line_with_newline)
            
            if current_tokens + line_tokens > max_chunk_size and current_chunk:
                chunk_content = header.format(part=part_num) + current_chunk
                chunks.append({
                    'content': chunk_content,
                    'files': [f"{file_path} (Part {part_num})"],
                    'token_count': self.count_tokens(chunk_content)
                })
                current_chunk = line_with_newline
                current_tokens = line_tokens
                part_num += 1
            else:
                current_chunk += line_with_newline
                current_tokens += line_tokens
        
        if current_chunk:
            chunk_content = header.format(part=part_num) + current_chunk
            chunks.append({
                'content': chunk_content,
                'files': [f"{file_path} (Part {part_num})"],
                'token_count': self.count_tokens(chunk_content)
            })
        
        return chunks

class LlamaClient:
    def __init__(self, api_key):
        self.api_key = api_key
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # UBS Internal AI API Configuration
        self.possible_endpoints = [
            "https://chat.copdev.azpriv-cloud.ubs.net/api/v1/messages",
            "https://chat.copdev.azpriv-cloud.ubs.net/api/v1/chat/completions",
            "https://chat.copdev.azpriv-cloud.ubs.net/v1/chat/completions",
            "https://api.copdev.azpriv-cloud.ubs.net/api/v1/messages",
            "https://api.copdev.azpriv-cloud.ubs.net/v1/chat/completions"
        ]
        
        self.base_url = self._find_working_endpoint()
        self.available_models = [
            "llama-4-scout-instruct",
            "llama-3.3-70b-instruct"
        ]
        print(f"Available UBS models: {self.available_models}")
    
    def _find_working_endpoint(self):
        """Find the working API endpoint"""
        print("Testing UBS Internal AI API endpoints...")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "User-Agent": "UBS-Documentation-Generator/1.0",
        }
        
        test_payload = {
            "model": "llama-4-scout-instruct",
            "messages": [
                {"role": "user", "content": "Hello, this is a connection test."}
            ],
            "max_tokens": 10,
            "temperature": 0.1
        }
        
        for endpoint in self.possible_endpoints:
            try:
                print(f"Testing endpoint: {endpoint}")
                
                options_response = requests.options(endpoint, headers=headers, timeout=10, verify=False)
                print(f"OPTIONS response for {endpoint}: {options_response.status_code}")
                
                if options_response.status_code in [200, 204, 405]:
                    response = requests.post(endpoint, headers=headers, json=test_payload, timeout=30, verify=False)
                    print(f"POST response for {endpoint}: {response.status_code}")
                    
                    if response.status_code == 200:
                        print(f"✓ Found working endpoint: {endpoint}")
                        return endpoint
                    elif response.status_code in [401, 403, 404, 405]:
                        continue
                        
            except Exception as e:
                print(f"Error testing {endpoint}: {str(e)}")
                continue
        
        print("⚠️ No working endpoint found. Using default endpoint.")
        return self.possible_endpoints[0]
    
    def _select_best_model(self):
        """Select the best available model"""
        preferred_models = [
            "llama-4-scout-instruct",
            "llama-3.3-70b-instruct"
        ]
        
        for model in preferred_models:
            if model in self.available_models:
                print(f"✓ Selected model: {model}")
                return model
        
        return "llama-4-scout-instruct"
    
    def count_tokens(self, text):
        """Count tokens in text"""
        return len(self.encoding.encode(text))
    
    def _make_api_request(self, prompt, max_tokens=2000, temperature=0.3):
        """Make API request with token management"""
        model = self._select_best_model()
        
        # Check token limits
        prompt_tokens = self.count_tokens(prompt)
        if prompt_tokens > MAX_TOKENS_PER_REQUEST * TOKEN_SAFETY_MARGIN:
            return f"Error: Prompt too long ({prompt_tokens} tokens). Please reduce content size."
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "User-Agent": "UBS-Documentation-Generator/1.0",
        }
        
        payload_formats = [
            {
                "model": model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": temperature
            },
            {
                "model": model,
                "prompt": prompt,
                "max_tokens": max_tokens,
                "temperature": temperature
            },
            {
                "prompt": prompt,
                "model": model,
                "max_tokens": max_tokens
            }
        ]
        
        last_error = None
        
        for i, payload in enumerate(payload_formats):
            try:
                print(f"Attempting API request with format {i+1}/{len(payload_formats)}")
                
                response = requests.post(self.base_url, headers=headers, json=payload, timeout=60, verify=False)
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"✓ Request successful with format {i+1}")
                    
                    if "choices" in result and len(result["choices"]) > 0:
                        return result["choices"][0]["message"]["content"]
                    elif "response" in result:
                        return result["response"]
                    elif "content" in result:
                        return result["content"]
                    elif "text" in result:
                        return result["text"]
                    else:
                        return f"Response received but format unclear: {result}"
                
                elif response.status_code == 405:
                    continue
                elif response.status_code == 401:
                    return "Authentication Error: Invalid API key."
                elif response.status_code == 403:
                    return "Access Forbidden: API key permissions issue."
                elif response.status_code == 429:
                    time.sleep(2)  # Rate limit backoff
                    continue
                else:
                    last_error = f"HTTP {response.status_code}: {response.text}"
                    continue
                    
            except Exception as e:
                last_error = f"Error with format {i+1}: {str(e)}"
                continue
        
        return last_error or "All request formats failed"
    
    def generate_documentation(self, code_chunk, project_name="", template_structure=None):
        """Generate documentation for code chunk with template guidance"""
        template_guidance = ""
        if template_structure:
            sections = template_structure.get('sections', [])
            template_guidance = f"""
Follow this template structure:
{chr(10).join([f"- {section.get('name', '')}: {section.get('description', '')}" for section in sections])}
"""
        
        prompt = f"""You are an expert technical documentation generator. Analyze the provided code and generate comprehensive documentation following the specified template structure.

Project: {project_name}
Files included: {', '.join(code_chunk['files'])}
Token count: {code_chunk.get('token_count', 'Unknown')}

{template_guidance}

Code Content:
{code_chunk['content']}

Generate detailed documentation that includes:
1. **Overview**: Brief description of what these files do
2. **Key Components**: Main classes, functions, and their purposes  
3. **Dependencies**: External libraries and frameworks used
4. **Code Structure**: How the code is organized
5. **Technical Details**: Important implementation details
6. **API Reference**: Document all public functions and classes
7. **Usage Examples**: Provide practical code examples

Format the response in clear markdown with proper headers and code examples where relevant.
Ensure all documented functions and classes actually exist in the provided code."""

        return self._make_api_request(prompt, max_tokens=3000, temperature=0.3)
    
    def generate_project_overview(self, project_info, all_files, template_structure=None):
        """Generate overall project documentation with template compliance"""
        files_list = "\n".join([f"- {f}" for f in all_files[:30]])
        if len(all_files) > 30:
            files_list += f"\n... and {len(all_files) - 30} more files"
        
        template_guidance = ""
        if template_structure:
            sections = template_structure.get('sections', [])
            template_guidance = f"""
IMPORTANT: Follow this exact template structure:
{chr(10).join([f"{i+1}. {section.get('name', '')}: {section.get('description', '')}" for i, section in enumerate(sections)])}
"""
        
        prompt = f"""Generate a comprehensive project README for this GitLab repository following the specified template structure.

Project Name: {project_info.get('name', 'Unknown')}
Description: {project_info.get('description', 'No description provided')}
Default Branch: {project_info.get('default_branch', 'main')}
Total Files: {len(all_files)}

{template_guidance}

Files in repository:
{files_list}

Create a professional README that follows the template structure exactly. Include:
- Project title and description
- Architecture overview
- Technology stack identification
- Installation instructions
- Usage guide with examples
- File structure explanation
- API documentation if applicable
- Contributing guidelines

Make it comprehensive, accurate, and suitable for both developers and stakeholders.
Ensure all sections from the template are included and properly formatted."""

        return self._make_api_request(prompt, max_tokens=4000, temperature=0.2)

class DocumentationGenerator:
    def __init__(self, verify_ssl=False):
        self.gitlab_client = GitLabClient(GITLAB_TOKEN, verify_ssl=verify_ssl)
        self.code_processor = CodeProcessor()
        self.llama_client = LlamaClient(LLAMA_API_KEY)
        self.file_manifest = {}
        self.processing_stats = {
            'total_files': 0,
            'processed_files': 0,
            'skipped_files': 0,
            'total_tokens': 0,
            'chunks_created': 0
        }
    
    def generate_documentation(self, repo_url, template_structure=None, status_callback=None):
        """Main function to generate complete documentation with coverage tracking"""
        try:
            # Step 1: Get project info
            if status_callback:
                status_callback("Connecting to GitLab...")
            project_info, gitlab_host = self.gitlab_client.get_project_info(repo_url)
            project_id = project_info['id']
            
            # Step 2: Get repository files with complete inventory
            if status_callback:
                status_callback("Fetching repository structure...")
            all_files = self.gitlab_client.get_repository_tree(gitlab_host, project_id)
            self.processing_stats['total_files'] = len(all_files)
            
            # Create file manifest for coverage tracking
            self.file_manifest = {
                'total_files': len(all_files),
                'all_files': [f['path'] for f in all_files],
                'filtered_files': [],
                'processed_files': [],
                'skipped_files': []
            }
            
            # Step 3: Filter and process files
            if status_callback:
                status_callback("Processing code files...")
            filtered_files = self.code_processor.filter_files(all_files)
            self.file_manifest['filtered_files'] = [f['path'] for f in filtered_files]
            
            # Step 4: Get file contents with progress tracking
            if status_callback:
                status_callback("Reading file contents...")
            files_content = {}
            max_files = min(MAX_FILES_TO_PROCESS, len(filtered_files))
            
            for i, file_info in enumerate(filtered_files[:max_files]):
                if status_callback:
                    status_callback(f"Reading file {i+1}/{max_files}: {file_info['path']}")
                
                content = self.gitlab_client.get_file_content(gitlab_host, project_id, file_info['path'])
                
                if not content.startswith("Error reading") and not content.startswith("File not found"):
                    files_content[file_info['path']] = content
                    self.file_manifest['processed_files'].append(file_info['path'])
                    self.processing_stats['processed_files'] += 1
                else:
                    self.file_manifest['skipped_files'].append(file_info['path'])
                    self.processing_stats['skipped_files'] += 1
            
            if not files_content:
                raise Exception("No readable files found in the repository")
            
            # Step 5: Generate project overview with template guidance
            if status_callback:
                status_callback("Generating project overview...")
            project_overview = self.llama_client.generate_project_overview(
                project_info, list(files_content.keys()), template_structure
            )
            
            # Step 6: Process code chunks with token counting
            if status_callback:
                status_callback("Processing code for AI analysis...")
            chunks = self.code_processor.chunk_content(files_content)
            self.processing_stats['chunks_created'] = len(chunks)
            self.processing_stats['total_tokens'] = sum(chunk.get('token_count', 0) for chunk in chunks)
            
            # Step 7: Generate documentation for each chunk
            documentation_parts = [project_overview]
            
            for i, chunk in enumerate(chunks):
                if status_callback:
                    status_callback(f"Generating documentation for chunk {i+1}/{len(chunks)}...")
                chunk_doc = self.llama_client.generate_documentation(
                    chunk, project_info.get('name', ''), template_structure
                )
                documentation_parts.append(f"\n\n## Code Analysis - Part {i+1}\n\n{chunk_doc}")
            
            # Step 8: Combine all documentation
            if status_callback:
                status_callback("Finalizing documentation...")
            
            final_documentation = "\n\n".join(documentation_parts)
            
            # Add comprehensive metadata footer
            coverage_percentage = (self.processing_stats['processed_files'] / self.processing_stats['total_files']) * 100
            final_documentation += f"\n\n---\n## Documentation Metadata"
            final_documentation += f"\n*Generated automatically from GitLab repository: {repo_url}*"
            final_documentation += f"\n*Total files in repository: {self.processing_stats['total_files']}*"
            final_documentation += f"\n*Files processed: {self.processing_stats['processed_files']}*"
            final_documentation += f"\n*Coverage: {coverage_percentage:.1f}%*"
            final_documentation += f"\n*Total tokens processed: {self.processing_stats['total_tokens']:,}*"
            final_documentation += f"\n*Chunks created: {self.processing_stats['chunks_created']}*"
            final_documentation += f"\n*Generated on: {project_info.get('last_activity_at', 'Unknown')}*"
            
            return final_documentation
            
        except Exception as e:
            raise Exception(f"Documentation generation failed: {str(e)}")
    
    def get_processing_stats(self):
        """Return processing statistics for evaluation"""
        return {
            **self.processing_stats,
            'file_manifest': self.file_manifest,
            'coverage_percentage': (self.processing_stats['processed_files'] / max(self.processing_stats['total_files'], 1)) * 100
        }

#coverage_tracker.py

import json
import os
from typing import Dict, List, Tuple
from collections import defaultdict
import tiktoken
from config import COVERAGE_REQUIREMENTS, MAX_TOKENS_PER_REQUEST

class FilesCoverageTracker:
    def __init__(self):
        self.total_files = 0
        self.processed_files = 0
        self.skipped_files = []
        self.critical_files = []
        self.token_counts = {}
        self.chunk_assignments = {}
        self.coverage_metrics = {}
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
    def create_file_manifest(self, repo_files: List[Dict]) -> Dict:
        """Create complete inventory of all repository files"""
        manifest = {
            'total_count': len(repo_files),
            'by_type': self._categorize_files(repo_files),
            'by_size': self._analyze_file_sizes(repo_files),
            'critical_files': self._identify_critical_files(repo_files),
            'estimated_tokens': self._estimate_total_tokens(repo_files)
        }
        
        self.total_files = len(repo_files)
        return manifest
    
    def _categorize_files(self, files: List[Dict]) -> Dict:
        """Categorize files by type and importance"""
        categories = defaultdict(list)
        
        for file_info in files:
            file_path = file_info['path']
            
            if file_path.endswith('.py'):
                categories['python'].append(file_path)
            elif file_path.endswith(('.js', '.ts', '.jsx', '.tsx')):
                categories['javascript'].append(file_path)
            elif file_path.endswith(('.java', '.kt')):
                categories['java'].append(file_path)
            elif file_path.endswith(('.cpp', '.c', '.h')):
                categories['cpp'].append(file_path)
            elif file_path.endswith('.md'):
                categories['documentation'].append(file_path)
            elif file_path.endswith(('.yml', '.yaml', '.json', '.xml')):
                categories['configuration'].append(file_path)
            elif file_path.endswith(('.sql', '.db')):
                categories['database'].append(file_path)
            else:
                categories['other'].append(file_path)
        
        return dict(categories)
    
    def _analyze_file_sizes(self, files: List[Dict]) -> Dict:
        """Analyze file sizes and estimate processing complexity"""
        size_analysis = {
            'small_files': [],    # < 1KB
            'medium_files': [],   # 1KB - 10KB
            'large_files': [],    # 10KB - 100KB
            'huge_files': []      # > 100KB
        }
        
        for file_info in files:
            size = file_info.get('size', 0)
            file_path = file_info['path']
            
            if size < 1024:
                size_analysis['small_files'].append(file_path)
            elif size < 10240:
                size_analysis['medium_files'].append(file_path)
            elif size < 102400:
                size_analysis['large_files'].append(file_path)
            else:
                size_analysis['huge_files'].append(file_path)
        
        return size_analysis
    
    def _identify_critical_files(self, files: List[Dict]) -> List[str]:
        """Identify critical files that must be documented"""
        critical_patterns = [
            'main.py', 'app.py', 'index.js', 'server.js', 'config.py',
            'settings.py', 'requirements.txt', 'package.json', 'README.md',
            'Dockerfile', 'docker-compose.yml', 'setup.py', '__init__.py'
        ]
        
        critical_files = []
        for file_info in files:
            file_path = file_info['path']
            if any(pattern in file_path.lower() for pattern in critical_patterns):
                critical_files.append(file_path)
                
        self.critical_files = critical_files
        return critical_files
    
    def _estimate_total_tokens(self, files: List[Dict]) -> int:
        """Estimate total tokens for all files"""
        # Rough estimation: 1 token per 4 characters
        total_size = sum(file_info.get('size', 0) for file_info in files)
        estimated_tokens = total_size // 4
        return estimated_tokens
    
    def track_file_processing(self, file_path: str, content: str, chunk_id: str = None):
        """Track processing of individual files"""
        token_count = len(self.encoding.encode(content))
        self.token_counts[file_path] = token_count
        
        if chunk_id:
            if chunk_id not in self.chunk_assignments:
                self.chunk_assignments[chunk_id] = []
            self.chunk_assignments[chunk_id].append(file_path)
        
        self.processed_files += 1
    
    def add_skipped_file(self, file_path: str, reason: str):
        """Track files that were skipped during processing"""
        self.skipped_files.append({
            'file_path': file_path,
            'reason': reason
        })
    
    def calculate_coverage_metrics(self) -> Dict:
        """Calculate comprehensive coverage metrics"""
        total_files = self.total_files
        processed_files = self.processed_files
        skipped_count = len(self.skipped_files)
        
        # Basic coverage metrics
        file_coverage = processed_files / total_files if total_files > 0 else 0
        critical_files_processed = sum(1 for cf in self.critical_files 
                                     if cf in self.token_counts)
        critical_coverage = critical_files_processed / len(self.critical_files) if self.critical_files else 1
        
        # Token efficiency metrics
        total_tokens = sum(self.token_counts.values())
        avg_tokens_per_file = total_tokens / processed_files if processed_files > 0 else 0
        token_efficiency = min(total_tokens / MAX_TOKENS_PER_REQUEST, 1.0)
        
        self.coverage_metrics = {
            'file_coverage': file_coverage,
            'critical_coverage': critical_coverage,
            'total_files': total_files,
            'processed_files': processed_files,
            'skipped_files': skipped_count,
            'total_tokens': total_tokens,
            'avg_tokens_per_file': avg_tokens_per_file,
            'token_efficiency': token_efficiency,
            'coverage_percentage': file_coverage * 100,
            'meets_requirements': self._check_coverage_requirements()
        }
        
        return self.coverage_metrics
    
    def _check_coverage_requirements(self) -> Dict:
        """Check if coverage meets minimum requirements"""
        requirements_met = {}
        
        file_coverage = self.processed_files / self.total_files if self.total_files > 0 else 0
        critical_files_processed = sum(1 for cf in self.critical_files 
                                     if cf in self.token_counts)
        critical_coverage = critical_files_processed / len(self.critical_files) if self.critical_files else 1
        
        requirements_met['min_file_coverage'] = file_coverage >= COVERAGE_REQUIREMENTS['min_file_coverage']
        requirements_met['critical_files_coverage'] = critical_coverage >= COVERAGE_REQUIREMENTS['critical_files_coverage']
        requirements_met['overall_pass'] = all(requirements_met.values())
        
        return requirements_met
    
    def generate_coverage_report(self) -> str:
        """Generate detailed coverage report"""
        metrics = self.calculate_coverage_metrics()
        
        report = f"""
# File Coverage Report

## Summary
- **Total Files**: {metrics['total_files']}
- **Processed Files**: {metrics['processed_files']}
- **Skipped Files**: {metrics['skipped_files']}
- **Coverage Percentage**: {metrics['coverage_percentage']:.1f}%
- **Critical Files Coverage**: {metrics['critical_coverage']:.1f}%

## Token Analysis
- **Total Tokens**: {metrics['total_tokens']:,}
- **Average Tokens per File**: {metrics['avg_tokens_per_file']:.0f}
- **Token Efficiency**: {metrics['token_efficiency']:.2f}

## Requirements Compliance
"""
        
        requirements = metrics['meets_requirements']
        for req, met in requirements.items():
            status = "✅ PASS" if met else "❌ FAIL"
            report += f"- **{req.replace('_', ' ').title()}**: {status}\n"
        
        if self.skipped_files:
            report += "\n## Skipped Files\n"
            for skipped in self.skipped_files:
                report += f"- `{skipped['file_path']}`: {skipped['reason']}\n"
        
        return report
    
    def export_coverage_data(self, output_path: str):
        """Export coverage data to JSON file"""
        coverage_data = {
            'metrics': self.coverage_metrics,
            'file_manifest': {
                'total_files': self.total_files,
                'processed_files': self.processed_files,
                'critical_files': self.critical_files,
                'skipped_files': self.skipped_files
            },
            'token_analysis': self.token_counts,
            'chunk_assignments': self.chunk_assignments
        }
        
        with open(output_path, 'w') as f:
            json.dump(coverage_data, f, indent=2)

class TokenLimitValidator:
    def __init__(self):
        self.encoding = tiktoken.get_encoding("cl100k_base")
        self.max_tokens = MAX_TOKENS_PER_REQUEST
        self.safety_margin = 0.75  # Use 75% of available tokens
        
    def validate_chunk_size(self, content: str) -> Tuple[bool, int]:
        """Validate if content fits within token limits"""
        token_count = len(self.encoding.encode(content))
        is_valid = token_count <= (self.max_tokens * self.safety_margin)
        return is_valid, token_count
    
    def suggest_chunk_split(self, content: str) -> List[str]:
        """Suggest how to split content if it exceeds token limits"""
        lines = content.split('\n')
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        max_chunk_tokens = int(self.max_tokens * self.safety_margin)
        
        for line in lines:
            line_tokens = len(self.encoding.encode(line + '\n'))
            
            if current_tokens + line_tokens > max_chunk_tokens and current_chunk:
                chunks.append(current_chunk)
                current_chunk = line + '\n'
                current_tokens = line_tokens
            else:
                current_chunk += line + '\n'
                current_tokens += line_tokens
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def analyze_token_distribution(self, chunks: List[Dict]) -> Dict:
        """Analyze token distribution across chunks"""
        token_counts = [chunk.get('token_count', 0) for chunk in chunks]
        
        return {
            'total_chunks': len(chunks),
            'total_tokens': sum(token_counts),
            'avg_tokens_per_chunk': sum(token_counts) / len(token_counts) if token_counts else 0,
            'max_tokens_per_chunk': max(token_counts) if token_counts else 0,
            'min_tokens_per_chunk': min(token_counts) if token_counts else 0,
            'token_efficiency': sum(token_counts) / (len(chunks) * self.max_tokens) if chunks else 0
        }

#evaluation.py

import numpy as np
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
import re
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import json
from config import QUALITY_THRESHOLDS

class DocumentationEvaluator:
    def __init__(self):
        self.metrics = {
            'template_compliance_f1': 0.0,
            'code_accuracy_f1': 0.0,
            'content_quality_score': 0.0,
            'bleu_score': 0.0,
            'rouge_scores': {},
            'overall_quality': 0.0
        }
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        
    def comprehensive_evaluation(self, generated_doc: str, reference_data: Dict) -> Dict:
        """Perform comprehensive evaluation of generated documentation"""
        
        # 1. Template compliance evaluation
        template_metrics = self.evaluate_template_compliance(
            generated_doc, reference_data.get('template_structure', {})
        )
        
        # 2. Code accuracy evaluation
        code_metrics = self.evaluate_code_accuracy(
            generated_doc, reference_data.get('source_code_analysis', {})
        )
        
        # 3. Content quality evaluation
        content_metrics = self.evaluate_content_quality(
            generated_doc, reference_data.get('reference_docs', [])
        )
        
        # 4. Calculate overall quality score
        overall_score = self.calculate_overall_quality(template_metrics, code_metrics, content_metrics)
        
        # Combine all metrics
        self.metrics.update({
            'template_compliance_f1': template_metrics['f1_score'],
            'code_accuracy_f1': code_metrics['f1_score'],
            'content_quality_score': content_metrics['quality_score'],
            'bleu_score': content_metrics.get('bleu_score', 0.0),
            'rouge_scores': content_metrics.get('rouge_scores', {}),
            'overall_quality': overall_score,
            'detailed_metrics': {
                'template': template_metrics,
                'code': code_metrics,
                'content': content_metrics
            }
        })
        
        return self.metrics
    
    def evaluate_template_compliance(self, generated_doc: str, template_structure: Dict) -> Dict:
        """Evaluate how well generated doc follows template structure"""
        if not template_structure:
            return {'f1_score': 0.0, 'confusion_matrix': None, 'details': 'No template structure provided'}
        
        sections = template_structure.get('sections', [])
        if not sections:
            return {'f1_score': 0.0, 'confusion_matrix': None, 'details': 'No sections in template'}
        
        # Create binary classification for each section
        actual_labels = []
        predicted_labels = []
        section_details = {}
        
        for section in sections:
            section_name = section['name']
            required = section.get('required', True)
            
            # Actual: 1 if required, 0 if optional
            actual_label = 1 if required else 0
            
            # Predicted: 1 if section exists in generated doc, 0 otherwise
            predicted_label = 1 if self._section_exists_in_doc(generated_doc, section_name) else 0
            
            actual_labels.append(actual_label)
            predicted_labels.append(predicted_label)
            
            section_details[section_name] = {
                'required': required,
                'found': bool(predicted_label),
                'content_type': section.get('content_type', 'text')
            }
        
        # Calculate confusion matrix and F1 score
        if actual_labels and predicted_labels:
            cm = confusion_matrix(actual_labels, predicted_labels, labels=[0, 1])
            f1 = f1_score(actual_labels, predicted_labels, average='weighted', zero_division=0)
            precision = precision_score(actual_labels, predicted_labels, average='weighted', zero_division=0)
            recall = recall_score(actual_labels, predicted_labels, average='weighted', zero_division=0)
        else:
            cm = np.array([[0, 0], [0, 0]])
            f1 = precision = recall = 0.0
        
        return {
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'confusion_matrix': cm.tolist(),
            'section_details': section_details,
            'sections_found': sum(predicted_labels),
            'sections_required': sum(actual_labels),
            'compliance_percentage': (sum(predicted_labels) / max(sum(actual_labels), 1)) * 100
        }
    
    def evaluate_code_accuracy(self, generated_doc: str, source_code_analysis: Dict) -> Dict:
        """Evaluate accuracy of code-related claims in documentation"""
        if not source_code_analysis:
            return {'f1_score': 0.0, 'confusion_matrix': None, 'details': 'No source code analysis provided'}
        
        # Extract code elements from source analysis
        actual_functions = source_code_analysis.get('functions', [])
        actual_classes = source_code_analysis.get('classes', [])
        actual_dependencies = source_code_analysis.get('dependencies', [])
        
        # Extract documented elements from generated doc
        documented_functions = self._extract_documented_functions(generated_doc)
        documented_classes = self._extract_documented_classes(generated_doc)
        documented_dependencies = self._extract_documented_dependencies(generated_doc)
        
        # Evaluate function documentation accuracy
        func_actual, func_predicted = self._create_binary_labels(actual_functions, documented_functions)
        
        # Evaluate class documentation accuracy
        class_actual, class_predicted = self._create_binary_labels(actual_classes, documented_classes)
        
        # Evaluate dependency documentation accuracy
        dep_actual, dep_predicted = self._create_binary_labels(actual_dependencies, documented_dependencies)
        
        # Combine all labels
        all_actual = func_actual + class_actual + dep_actual
        all_predicted = func_predicted + class_predicted + dep_predicted
        
        if all_actual and all_predicted:
            cm = confusion_matrix(all_actual, all_predicted, labels=[0, 1])
            f1 = f1_score(all_actual, all_predicted, average='weighted', zero_division=0)
            precision = precision_score(all_actual, all_predicted, average='weighted', zero_division=0)
            recall = recall_score(all_actual, all_predicted, average='weighted', zero_division=0)
        else:
            cm = np.array([[0, 0], [0, 0]])
            f1 = precision = recall = 0.0
        
        return {
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'confusion_matrix': cm.tolist(),
            'function_accuracy': len(set(actual_functions) & set(documented_functions)) / max(len(actual_functions), 1),
            'class_accuracy': len(set(actual_classes) & set(documented_classes)) / max(len(actual_classes), 1),
            'dependency_accuracy': len(set(actual_dependencies) & set(documented_dependencies)) / max(len(actual_dependencies), 1),
            'details': {
                'functions': {'actual': len(actual_functions), 'documented': len(documented_functions)},
                'classes': {'actual': len(actual_classes), 'documented': len(documented_classes)},
                'dependencies': {'actual': len(actual_dependencies), 'documented': len(documented_dependencies)}
            }
        }
    
    def evaluate_content_quality(self, generated_doc: str, reference_docs: List[str]) -> Dict:
        """Evaluate content quality using BLEU and ROUGE scores"""
        if not reference_docs:
            return {
                'quality_score': 0.7,  # Default score when no reference
                'bleu_score': 0.0,
                'rouge_scores': {},
                'details': 'No reference documentation provided'
            }
        
        # Calculate BLEU score
        bleu_scores = []
        for ref_doc in reference_docs:
            ref_tokens = ref_doc.split()
            gen_tokens = generated_doc.split()
            bleu = sentence_bleu([ref_tokens], gen_tokens)
            bleu_scores.append(bleu)
        
        avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0
        
        # Calculate ROUGE scores
        rouge_scores = {}
        for ref_doc in reference_docs:
            scores = self.rouge_scorer.score(ref_doc, generated_doc)
            for metric, score in scores.items():
                if metric not in rouge_scores:
                    rouge_scores[metric] = []
                rouge_scores[metric].append(score.fmeasure)
        
        # Average ROUGE scores
        avg_rouge_scores = {metric: np.mean(scores) for metric, scores in rouge_scores.items()}
        
        # Calculate overall content quality score
        quality_score = (avg_bleu * 0.3 + 
                        avg_rouge_scores.get('rouge1', 0) * 0.3 + 
                        avg_rouge_scores.get('rougeL', 0) * 0.4)
        
        return {
            'quality_score': quality_score,
            'bleu_score': avg_bleu,
            'rouge_scores': avg_rouge_scores,
            'readability_score': self._calculate_readability_score(generated_doc),
            'completeness_score': self._calculate_completeness_score(generated_doc)
        }
    
    def calculate_overall_quality(self, template_metrics: Dict, code_metrics: Dict, content_metrics: Dict) -> float:
        """Calculate weighted overall quality score"""
        weights = {
            'template_compliance': 0.3,
            'code_accuracy': 0.4,
            'content_quality': 0.3
        }
        
        template_score = template_metrics.get('f1_score', 0.0)
        code_score = code_metrics.get('f1_score', 0.0)
        content_score = content_metrics.get('quality_score', 0.0)
        
        overall = (template_score * weights['template_compliance'] + 
                  code_score * weights['code_accuracy'] + 
                  content_score * weights['content_quality'])
        
        return overall
    
    def _section_exists_in_doc(self, doc: str, section_name: str) -> bool:
        """Check if a section exists in the generated documentation"""
        patterns = [
            f"# {section_name}",
            f"## {section_name}",
            f"### {section_name}",
            f"**{section_name}**",
            f"{section_name}:",
            section_name
        ]
        
        doc_lower = doc.lower()
        section_lower = section_name.lower()
        
        for pattern in patterns:
            if pattern.lower() in doc_lower:
                return True
        
        # Fuzzy matching
        words = section_lower.split()
        if len(words) > 1:
            for word in words:
                if len(word) > 3 and word in doc_lower:
                    return True
        
        return False
    
    def _extract_documented_functions(self, doc: str) -> List[str]:
        """Extract function names mentioned in documentation"""
        patterns = [
            r'def\s+(\w+)\s*\(',
            r'function\s+(\w+)\s*\(',
            r'`(\w+)\(\)`',
            r'(\w+)\(\)',
        ]
        
        functions = set()
        for pattern in patterns:
            matches = re.findall(pattern, doc)
            functions.update(matches)
        
        return list(functions)
    
    def _extract_documented_classes(self, doc: str) -> List[str]:
        """Extract class names mentioned in documentation"""
        patterns = [
            r'class\s+(\w+)',
            r'`(\w+)`\s+class',
            r'(\w+)\s+class',
        ]
        
        classes = set()
        for pattern in patterns:
            matches = re.findall(pattern, doc)
            classes.update(matches)
        
        return list(classes)
    
    def _extract_documented_dependencies(self, doc: str) -> List[str]:
        """Extract dependencies mentioned in documentation"""
        patterns = [
            r'import\s+(\w+)',
            r'from\s+(\w+)',
            r'require\([\'"](\w+)[\'"]\)',
            r'`(\w+)`\s+(?:library|package|module)',
        ]
        
        dependencies = set()
        for pattern in patterns:
            matches = re.findall(pattern, doc)
            dependencies.update(matches)
        
        return list(dependencies)
    
    def _create_binary_labels(self, actual_items: List[str], documented_items: List[str]) -> Tuple[List[int], List[int]]:
        """Create binary labels for actual vs predicted items"""
        actual_labels = []
        predicted_labels = []
        
        # For each actual item, check if it's documented
        for item in actual_items:
            actual_labels.append(1)  # Item exists in code
            predicted_labels.append(1 if item in documented_items else 0)
        
        return actual_labels, predicted_labels
    
    def _calculate_readability_score(self, doc: str) -> float:
        """Calculate readability score based on text characteristics"""
        sentences = doc.split('.')
        words = doc.split()
        
        if not sentences or not words:
            return 0.0
        
        avg_sentence_length = len(words) / len(sentences)
        
        # Simple readability score (inverse of average sentence length, normalized)
        readability = max(0, 1 - (avg_sentence_length - 15) / 50)
        return min(1.0, readability)
    
    def _calculate_completeness_score(self, doc: str) -> float:
        """Calculate completeness score based on content coverage"""
        # Check for presence of key documentation elements
        elements = [
            'overview', 'installation', 'usage', 'example', 'api',
            'function', 'class', 'parameter', 'return', 'import'
        ]
        
        doc_lower = doc.lower()
        found_elements = sum(1 for element in elements if element in doc_lower)
        
        return found_elements / len(elements)
    
    def generate_evaluation_report(self) -> str:
        """Generate comprehensive evaluation report"""
        report = f"""
# Documentation Quality Evaluation Report

## Overall Quality Score: {self.metrics['overall_quality']:.2f}

## Detailed Metrics

### Template Compliance
- **F1 Score**: {self.metrics['template_compliance_f1']:.3f}
- **Status**: {'✅ PASS' if self.metrics['template_compliance_f1'] >= QUALITY_THRESHOLDS['template_compliance_f1'] else '❌ FAIL'}

### Code Accuracy
- **F1 Score**: {self.metrics['code_accuracy_f1']:.3f}
- **Status**: {'✅ PASS' if self.metrics['code_accuracy_f1'] >= QUALITY_THRESHOLDS['code_accuracy_f1'] else '❌ FAIL'}

### Content Quality
- **Quality Score**: {self.metrics['content_quality_score']:.3f}
- **BLEU Score**: {self.metrics['bleu_score']:.3f}
- **Status**: {'✅ PASS' if self.metrics['content_quality_score'] >= QUALITY_THRESHOLDS['content_quality'] else '❌ FAIL'}

## Quality Gate Status
Overall Status: {'✅ PASS' if self.metrics['overall_quality'] >= QUALITY_THRESHOLDS['overall_quality'] else '❌ FAIL'}
"""
        
        return report
    
    def export_metrics(self, output_path: str):
        """Export evaluation metrics to JSON file"""
        with open(output_path, 'w') as f:
            json.dump(self.metrics, f, indent=2, default=str)


#template.py
import json
import os
import PyPDF2
import pdfplumber
from docx import Document
import re
from typing import Dict, List, Optional
from pathlib import Path

class TemplateParser:
    def __init__(self, templates_dir: str = "templates"):
        self.templates_dir = templates_dir
        self.supported_formats = ['.pdf', '.docx']
        
    def parse_template_document(self, template_path: str) -> Dict:
        """Parse PDF or DOCX template and extract structure"""
        if not os.path.exists(template_path):
            raise FileNotFoundError(f"Template file not found: {template_path}")
        
        file_extension = Path(template_path).suffix.lower()
        
        if file_extension == '.pdf':
            return self._parse_pdf_template(template_path)
        elif file_extension == '.docx':
            return self._parse_docx_template(template_path)
        else:
            raise ValueError(f"Unsupported template format: {file_extension}")
    
    def _parse_pdf_template(self, pdf_path: str) -> Dict:
        """Extract structure from PDF template"""
        structure = {
            'document_info': {},
            'sections': [],
            'formatting_rules': {},
            'content_requirements': {}
        }
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                full_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"
                
                structure['document_info'] = {
                    'total_pages': len(pdf.pages),
                    'title': self._extract_title_from_text(full_text),
                    'template_type': 'pdf'
                }
                
                structure['sections'] = self._extract_sections_from_text(full_text)
                structure['formatting_rules'] = self._analyze_pdf_formatting(pdf)
                
        except Exception as e:
            print(f"Error parsing PDF template: {str(e)}")
            structure = self._create_default_structure()
        
        return structure
    
    def _parse_docx_template(self, docx_path: str) -> Dict:
        """Extract structure from DOCX template"""
        structure = {
            'document_info': {},
            'sections': [],
            'formatting_rules': {},
            'content_requirements': {}
        }
        
        try:
            doc = Document(docx_path)
            
            structure['document_info'] = {
                'total_paragraphs': len(doc.paragraphs),
                'title': self._extract_title_from_docx(doc),
                'template_type': 'docx'
            }
            
            structure['sections'] = self._extract_sections_from_docx(doc)
            structure['formatting_rules'] = self._analyze_docx_formatting(doc)
            
        except Exception as e:
            print(f"Error parsing DOCX template: {str(e)}")
            structure = self._create_default_structure()
        
        return structure
    
    def _extract_title_from_text(self, text: str) -> str:
        """Extract document title from text"""
        lines = text.split('\n')
        for line in lines[:10]:
            line = line.strip()
            if line and len(line) < 100:
                return line
        return "Documentation Template"
    
    def _extract_title_from_docx(self, doc: Document) -> str:
        """Extract title from DOCX document"""
        for paragraph in doc.paragraphs[:5]:
            if paragraph.text.strip() and len(paragraph.text) < 100:
                return paragraph.text.strip()
        return "Documentation Template"
    
    def _extract_sections_from_text(self, text: str) -> List[Dict]:
        """Extract sections and headings from text"""
        sections = []
        lines = text.split('\n')
        
        heading_patterns = [
            r'^#+ (.+)$',
            r'^(\d+\.?\s+.+)$',
            r'^([A-Z][A-Za-z\s]+)$',
            r'^(.+):$',
        ]
        
        section_order = 1
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            for pattern in heading_patterns:
                match = re.match(pattern, line)
                if match:
                    section_name = match.group(1).strip()
                    if len(section_name) > 3 and len(section_name) < 100:
                        sections.append({
                            'name': section_name,
                            'required': True,
                            'order': section_order,
                            'content_type': self._determine_content_type(section_name),
                            'description': self._generate_section_description(section_name)
                        })
                        section_order += 1
                    break
        
        if not sections:
            sections = self._create_default_sections()
        
        return sections
    
    def _extract_sections_from_docx(self, doc: Document) -> List[Dict]:
        """Extract sections from DOCX document"""
        sections = []
        section_order = 1
        
        for paragraph in doc.paragraphs:
            text = paragraph.text.strip()
            if not text:
                continue
            
            if self._is_likely_heading(paragraph, text):
                sections.append({
                    'name': text,
                    'required': True,
                    'order': section_order,
                    'content_type': self._determine_content_type(text),
                    'description': self._generate_section_description(text)
                })
                section_order += 1
        
        if not sections:
            sections = self._create_default_sections()
        
        return sections
    
    def _is_likely_heading(self, paragraph, text: str) -> bool:
        """Determine if a paragraph is likely a heading"""
        if paragraph.style.name.startswith('Heading'):
            return True
        
        if (len(text) < 100 and 
            len(text) > 3 and 
            not text.endswith('.') and
            (text.isupper() or text.istitle())):
            return True
        
        return False
    
    def _determine_content_type(self, section_name: str) -> str:
        """Determine expected content type for a section"""
        section_lower = section_name.lower()
        
        if any(word in section_lower for word in ['overview', 'introduction', 'summary']):
            return 'text'
        elif any(word in section_lower for word in ['installation', 'setup', 'getting started']):
            return 'numbered_list'
        elif any(word in section_lower for word in ['api', 'reference', 'functions']):
            return 'table'
        elif any(word in section_lower for word in ['example', 'usage', 'tutorial']):
            return 'code_blocks'
        elif any(word in section_lower for word in ['requirements', 'dependencies']):
            return 'bullet_list'
        else:
            return 'text'
    
    def _generate_section_description(self, section_name: str) -> str:
        """Generate description for a section based on its name"""
        section_lower = section_name.lower()
        
        descriptions = {
            'overview': 'High-level description of the project and its purpose',
            'introduction': 'Introduction to the project and its goals',
            'installation': 'Step-by-step installation instructions',
            'setup': 'Configuration and setup procedures',
            'usage': 'How to use the software with examples',
            'api': 'Complete API documentation with parameters and return values',
            'examples': 'Practical code examples and use cases',
            'requirements': 'System requirements and dependencies',
            'configuration': 'Configuration options and settings',
            'troubleshooting': 'Common issues and solutions',
            'contributing': 'Guidelines for contributing to the project'
        }
        
        for key, desc in descriptions.items():
            if key in section_lower:
                return desc
        
        return f"Documentation for {section_name}"
    
    def _analyze_pdf_formatting(self, pdf) -> Dict:
        """Analyze formatting patterns in PDF"""
        return {
            'heading_style': 'markdown',
            'code_blocks': True,
            'table_support': True,
            'image_support': True,
            'max_line_length': 80
        }
    
    def _analyze_docx_formatting(self, doc: Document) -> Dict:
        """Analyze formatting patterns in DOCX"""
        return {
            'heading_style': 'docx_native',
            'code_blocks': True,
            'table_support': True,
            'image_support': True,
            'preserve_styling': True
        }
    
    def _create_default_structure(self) -> Dict:
        """Create default template structure if parsing fails"""
        return {
            'document_info': {
                'title': 'Software Documentation',
                'template_type': 'default'
            },
            'sections': self._create_default_sections(),
            'formatting_rules': {
                'heading_style': 'markdown',
                'code_blocks': True,
                'table_support': True
            }
        }
    
    def _create_default_sections(self) -> List[Dict]:
        """Create default sections for documentation"""
        return [
            {
                'name': 'Project Overview',
                'required': True,
                'order': 1,
                'content_type': 'text',
                'description': 'High-level project description and purpose'
            },
            {
                'name': 'Installation Guide',
                'required': True,
                'order': 2,
                'content_type': 'numbered_list',
                'description': 'Step-by-step installation instructions'
            },
            {
                'name': 'Usage Examples',
                'required': True,
                'order': 3,
                'content_type': 'code_blocks',
                'description': 'Practical code examples and use cases'
            },
            {
                'name': 'API Reference',
                'required': True,
                'order': 4,
                'content_type': 'table',
                'description': 'Complete API documentation'
            },
            {
                'name': 'Configuration',
                'required': False,
                'order': 5,
                'content_type': 'text',
                'description': 'Configuration options and settings'
            }
        ]

class TemplateValidator:
    def __init__(self):
        self.validation_rules = {}
        
    def validate_template_compliance(self, generated_doc: str, template_structure: Dict) -> Dict:
        """Validate generated documentation against template structure"""
        results = {
            'overall_compliance': 0.0,
            'section_compliance': {},
            'missing_sections': [],
            'extra_sections': [],
            'formatting_compliance': 0.0
        }
        
        required_sections = [s for s in template_structure.get('sections', []) if s.get('required', True)]
        
        sections_found = 0
        for section in required_sections:
            section_name = section['name']
            if self._section_exists_in_doc(generated_doc, section_name):
                results['section_compliance'][section_name] = True
                sections_found += 1
            else:
                results['section_compliance'][section_name] = False
                results['missing_sections'].append(section_name)
        
        if required_sections:
            results['overall_compliance'] = sections_found / len(required_sections)
        
        results['formatting_compliance'] = self._check_formatting_compliance(
            generated_doc, template_structure.get('formatting_rules', {})
        )
        
        return results
    
    def _section_exists_in_doc(self, doc: str, section_name: str) -> bool:
        """Check if a section exists in the generated documentation"""
        patterns = [
            f"# {section_name}",
            f"## {section_name}",
            f"### {section_name}",
            f"**{section_name}**",
            f"{section_name}:",
            section_name
        ]
        
        doc_lower = doc.lower()
        section_lower = section_name.lower()
        
        for pattern in patterns:
            if pattern.lower() in doc_lower:
                return True
        
        words = section_lower.split()
        if len(words) > 1:
            for word in words:
                if len(word) > 3 and word in doc_lower:
                    return True
        
        return False
    
    def _check_formatting_compliance(self, doc: str, formatting_rules: Dict) -> float:
        """Check formatting compliance"""
        compliance_score = 0.0
        total_checks = 0
        
        if formatting_rules.get('heading_style') == 'markdown':
            if '# ' in doc or '## ' in doc:
                compliance_score += 1
            total_checks += 1
        
        if formatting_rules.get('code_blocks'):
            if '```
                compliance_score += 1
            total_checks += 1
        
        if formatting_rules.get('table_support'):
            if '|' in doc and '---' in doc:
                compliance_score += 1
            total_checks += 1
        
        return compliance_score / total_checks if total_checks > 0 else 1.0

class JSONGenerator:
    def __init__(self):
        pass
        
    def create_template_json(self, template_structure: Dict, output_path: str):
        """Create JSON structure file from parsed template"""
        json_structure = {
            'template_info': template_structure.get('document_info', {}),
            'sections': template_structure.get('sections', []),
            'formatting_rules': template_structure.get('formatting_rules', {}),
            'quality_requirements': {
                'min_sections': len([s for s in template_structure.get('sections', []) if s.get('required', True)]),
                'code_examples_required': any('code' in s.get('content_type', '') for s in template_structure.get('sections', [])),
                'technical_accuracy': 'high',
                'readability_level': 'intermediate'
            },
            'generation_guidance': self._create_generation_guidance(template_structure)
        }
        
        with open(output_path, 'w') as f:
            json.dump(json_structure, f, indent=2)
        
        return json_structure
    
    def _create_generation_guidance(self, template_structure: Dict) -> Dict:
        """Create guidance for LLM generation"""
        sections = template_structure.get('sections', [])
        
        return {
            'section_order': [s['name'] for s in sorted(sections, key=lambda x: x.get('order', 0))],
            'content_mapping': {
                s['name']: {
                    'type': s.get('content_type', 'text'),
                    'description': s.get('description', ''),
                    'required': s.get('required', True)
                }
                for s in sections
            },
            'formatting_instructions': template_structure.get('formatting_rules', {}),
            'quality_checks': [
                'Verify all required sections are present',
                'Ensure content matches expected types',
                'Validate code examples against source',
                'Check formatting consistency'
            ]
        }

def load_template_structure(templates_dir: str = "templates") -> Optional[Dict]:
    """Load template structure from available template files"""
    parser = TemplateParser(templates_dir)
    json_generator = JSONGenerator()
    
    template_files = []
    if os.path.exists(templates_dir):
        for file in os.listdir(templates_dir):
            if file.endswith(('.pdf', '.docx')):
                template_files.append(os.path.join(templates_dir, file))
    
    if not template_files:
        print("No template files found. Using default structure.")
        return None
    
    template_path = template_files
    print(f"Loading template: {template_path}")
    
    try:
        structure = parser.parse_template_document(template_path)
        
        json_path = os.path.join(templates_dir, "template_structure.json")
        json_generator.create_template_json(structure, json_path)
        
        print(f"Template structure created: {json_path}")
        return structure
        
    except Exception as e:
        print(f"Error loading template: {str(e)}")
        return None


#export.py

import os
import markdown
from weasyprint import HTML, CSS
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.style import WD_STYLE_TYPE
import re
from typing import Dict, Optional
from pathlib import Path

class PDFExporter:
    def __init__(self):
        self.styles = getSampleStyleSheet()
        self._setup_custom_styles()
    
    def _setup_custom_styles(self):
        """Setup custom styles for PDF generation"""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            spaceAfter=30,
            alignment=1,
            textColor=colors.darkblue
        ))
        
        self.styles.add(ParagraphStyle(
            name='CustomHeading1',
            parent=self.styles['Heading1'],
            fontSize=18,
            spaceAfter=12,
            textColor=colors.darkblue,
            borderWidth=1,
            borderColor=colors.darkblue,
            borderPadding=5
        ))
        
        self.styles.add(ParagraphStyle(
            name='Code',
            parent=self.styles['Normal'],
            fontName='Courier',
            fontSize=10,
            leftIndent=20,
            backgroundColor=colors.lightgrey,
            borderWidth=1,
            borderColor=colors.grey,
            borderPadding=5
        ))
    
    def export_to_pdf(self, content: str, output_path: str, template_info: Optional[Dict] = None) -> bool:
        """Export markdown content to PDF"""
        try:
            doc = SimpleDocTemplate(output_path, pagesize=A4)
            story = []
            
            if template_info:
                story.extend(self._create_title_page(template_info))
            
            story.extend(self._markdown_to_pdf_elements(content))
            
            doc.build(story)
            return True
            
        except Exception as e:
            print(f"Error exporting to PDF: {str(e)}")
            return False
    
    def _create_title_page(self, template_info: Dict) -> list:
        """Create title page for PDF"""
        story = []
        
        title = template_info.get('title', 'Documentation')
        story.append(Paragraph(title, self.styles['CustomTitle']))
        story.append(Spacer(1, 0.5*inch))
        
        metadata = [
            ['Generated from:', template_info.get('repo_url', 'Unknown')],
            ['Generated on:', template_info.get('generated_date', 'Unknown')],
            ['Total files:', str(template_info.get('total_files', 0))],
            ['Coverage:', f"{template_info.get('coverage', 0):.1f}%"]
        ]
        
        table = Table(metadata, colWidths=[2*inch, 4*inch])
        table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, -1), colors.lightgrey),
            ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
            ('FONTSIZE', (0, 0), (-1, -1), 12),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
            ('BACKGROUND', (0, 0), (0, -1), colors.grey),
            ('TEXTCOLOR', (0, 0), (0, -1), colors.whitesmoke),
        ]))
        
        story.append(table)
        story.append(Spacer(1, 1*inch))
        
        return story
    
    def _markdown_to_pdf_elements(self, content: str) -> list:
        """Convert markdown content to PDF elements"""
        story = []
        lines = content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
            
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                text = line.lstrip('#').strip()
                
                if level == 1:
                    story.append(Paragraph(text, self.styles['CustomHeading1']))
                elif level == 2:
                    story.append(Paragraph(text, self.styles['Heading2']))
                else:
                    story.append(Paragraph(text, self.styles['Heading3']))
                
                story.append(Spacer(1, 12))
            
            elif line.startswith('```'):
                i += 1
                code_lines = []
                while i < len(lines) and not lines[i].strip().startswith('```
                    code_lines.append(lines[i])
                    i += 1
                
                code_content = '\n'.join(code_lines)
                story.append(Paragraph(code_content, self.styles['Code']))
                story.append(Spacer(1, 12))
            
            else:
                para_lines = [line]
                i += 1
                while i < len(lines) and lines[i].strip() and not lines[i].startswith('#') and not lines[i].startswith('```'):
                    para_lines.append(lines[i].strip())
                    i += 1
                
                para_text = ' '.join(para_lines)
                story.append(Paragraph(para_text, self.styles['Normal']))
                story.append(Spacer(1, 12))
                continue
            
            i += 1
        
        return story

class DOCXExporter:
    def __init__(self):
        self.doc = None
        
    def export_to_docx(self, content: str, output_path: str, template_info: Optional[Dict] = None) -> bool:
        """Export markdown content to DOCX"""
        try:
            self.doc = Document()
            
            if template_info:
                self._create_docx_title_page(template_info)
            
            self._markdown_to_docx(content)
            
            self.doc.save(output_path)
            return True
            
        except Exception as e:
            print(f"Error exporting to DOCX: {str(e)}")
            return False
    
    def _create_docx_title_page(self, template_info: Dict):
        """Create title page for DOCX"""
        title = self.doc.add_heading(template_info.get('title', 'Documentation'), 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        self.doc.add_paragraph()
        metadata_para = self.doc.add_paragraph()
        metadata_para.add_run('Generated from: ').bold = True
        metadata_para.add_run(template_info.get('repo_url', 'Unknown'))
        
        date_para = self.doc.add_paragraph()
        date_para.add_run('Generated on: ').bold = True
        date_para.add_run(template_info.get('generated_date', 'Unknown'))
        
        files_para = self.doc.add_paragraph()
        files_para.add_run('Total files: ').bold = True
        files_para.add_run(str(template_info.get('total_files', 0)))
        
        coverage_para = self.doc.add_paragraph()
        coverage_para.add_run('Coverage: ').bold = True
        coverage_para.add_run(f"{template_info.get('coverage', 0):.1f}%")
        
        self.doc.add_page_break()
    
    def _markdown_to_docx(self, content: str):
        """Convert markdown content to DOCX"""
        lines = content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
            
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                text = line.lstrip('#').strip()
                self.doc.add_heading(text, level)
            
            elif line.startswith('```
                i += 1
                code_lines = []
                while i < len(lines) and not lines[i].strip().startswith('```'):
                    code_lines.append(lines[i])
                    i += 1
                
                code_content = '\n'.join(code_lines)
                code_para = self.doc.add_paragraph(code_content)
                code_para.style = 'Intense Quote'
            
            elif line.startswith('- ') or line.startswith('* '):
                text = line[2:].strip()
                self.doc.add_paragraph(text, style='List Bullet')
            
            elif re.match(r'^\d+\.\s', line):
                text = re.sub(r'^\d+\.\s', '', line)
                self.doc.add_paragraph(text, style='List Number')
            
            else:
                para_lines = [line]
                i += 1
                while i < len(lines) and lines[i].strip() and not lines[i].startswith('#') and not lines[i].startswith('```
                    para_lines.append(lines[i].strip())
                    i += 1
                
                para_text = ' '.join(para_lines)
                self.doc.add_paragraph(para_text)
                continue
            
            i += 1

class MarkdownExporter:
    def __init__(self):
        pass
    
    def export_to_markdown(self, content: str, output_path: str, template_info: Optional[Dict] = None) -> bool:
        """Export content to markdown file with metadata"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                if template_info:
                    f.write(self._create_markdown_metadata(template_info))
                
                f.write(content)
            
            return True
            
        except Exception as e:
            print(f"Error exporting to Markdown: {str(e)}")
            return False
    
    def _create_markdown_metadata(self, template_info: Dict) -> str:
        """Create metadata header for markdown"""
        metadata = f"""---
title: {template_info.get('title', 'Documentation')}
generated_from: {template_info.get('repo_url', 'Unknown')}
generated_date: {template_info.get('generated_date', 'Unknown')}
total_files: {template_info.get('total_files', 0)}
coverage: {template_info.get('coverage', 0):.1f}%
---

"""
        return metadata

class BatchExporter:
    def __init__(self):
        self.pdf_exporter = PDFExporter()
        self.docx_exporter = DOCXExporter()
        self.markdown_exporter = MarkdownExporter()
    
    def export_all_formats(self, content: str, base_filename: str, template_info: Optional[Dict] = None) -> Dict[str, bool]:
        """Export content to all supported formats"""
        results = {}
        
        pdf_path = f"{base_filename}.pdf"
        results['pdf'] = self.pdf_exporter.export_to_pdf(content, pdf_path, template_info)
        
        docx_path = f"{base_filename}.docx"
        results['docx'] = self.docx_exporter.export_to_docx(content, docx_path, template_info)
        
        md_path = f"{base_filename}.md"
        results['markdown'] = self.markdown_exporter.export_to_markdown(content, md_path, template_info)
        
        return results
    
    def export_single_format(self, content: str, output_path: str, format_type: str, template_info: Optional[Dict] = None) -> bool:
        """Export content to a single format"""
        format_type = format_type.lower()
        
        if format_type == 'pdf':
            return self.pdf_exporter.export_to_pdf(content, output_path, template_info)
        elif format_type in ['docx', 'doc']:
            return self.docx_exporter.export_to_docx(content, output_path, template_info)
        elif format_type in ['md', 'markdown']:
            return self.markdown_exporter.export_to_markdown(content, output_path, template_info)
        else:
            print(f"Unsupported format: {format_type}")
            return False

#utils.py

import os
import re
import json
import tiktoken
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import ast
from collections import defaultdict

class FileTracker:
    def __init__(self):
        self.file_inventory = {}
        self.processing_log = []
        
    def create_inventory(self, files_list: List[Dict]) -> Dict:
        """Create comprehensive file inventory"""
        inventory = {
            'total_files': len(files_list),
            'by_extension': defaultdict(list),
            'by_size': {'small': [], 'medium': [], 'large': [], 'huge': []},
            'critical_files': [],
            'documentation_files': [],
            'configuration_files': [],
            'source_files': []
        }
        
        for file_info in files_list:
            file_path = file_info['path']
            file_size = file_info.get('size', 0)
            
            extension = Path(file_path).suffix.lower()
            inventory['by_extension'][extension].append(file_path)
            
            if file_size < 1024:
                inventory['by_size']['small'].append(file_path)
            elif file_size < 10240:
                inventory['by_size']['medium'].append(file_path)
            elif file_size < 102400:
                inventory['by_size']['large'].append(file_path)
            else:
                inventory['by_size']['huge'].append(file_path)
            
            if self._is_critical_file(file_path):
                inventory['critical_files'].append(file_path)
            elif self._is_documentation_file(file_path):
                inventory['documentation_files'].append(file_path)
            elif self._is_configuration_file(file_path):
                inventory['configuration_files'].append(file_path)
            elif self._is_source_file(file_path):
                inventory['source_files'].append(file_path)
        
        self.file_inventory = inventory
        return inventory
    
    def _is_critical_file(self, file_path: str) -> bool:
        """Check if file is critical for documentation"""
        critical_patterns = [
            'main.py', 'app.py', 'index.js', 'server.js',
            '__init__.py', 'setup.py', 'requirements.txt',
            'package.json', 'Dockerfile', 'README.md'
        ]
        return any(pattern in file_path.lower() for pattern in critical_patterns)
    
    def _is_documentation_file(self, file_path: str) -> bool:
        """Check if file is documentation"""
        doc_extensions = ['.md', '.rst', '.txt']
        return any(file_path.lower().endswith(ext) for ext in doc_extensions)
    
    def _is_configuration_file(self, file_path: str) -> bool:
        """Check if file is configuration"""
        config_extensions = ['.yml', '.yaml', '.json', '.xml', '.ini', '.cfg']
        config_patterns = ['config', 'settings', '.env']
        return (any(file_path.lower().endswith(ext) for ext in config_extensions) or
                any(pattern in file_path.lower() for pattern in config_patterns))
    
    def _is_source_file(self, file_path: str) -> bool:
        """Check if file is source code"""
        source_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.h', '.ts', '.jsx', '.tsx']
        return any(file_path.lower().endswith(ext) for ext in source_extensions)
    
    def log_processing(self, file_path: str, status: str, details: str = ""):
        """Log file processing status"""
        self.processing_log.append({
            'file_path': file_path,
            'status': status,
            'details': details,
            'timestamp': self._get_timestamp()
        })
    
    def _get_timestamp(self) -> str:
        """Get current timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def get_processing_summary(self) -> Dict:
        """Get summary of processing results"""
        statuses = defaultdict(int)
        for log_entry in self.processing_log:
            statuses[log_entry['status']] += 1
        
        return {
            'total_processed': len(self.processing_log),
            'status_breakdown': dict(statuses),
            'success_rate': statuses['success'] / len(self.processing_log) if self.processing_log else 0
        }

class TokenCounter:
    def __init__(self):
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        return len(self.encoding.encode(text))
    
    def estimate_cost(self, token_count: int, model: str = "llama-4") -> float:
        """Estimate API cost based on token count"""
        cost_per_1k_tokens = {
            "llama-4": 0.03,
            "llama-3.3": 0.02
        }
        
        rate = cost_per_1k_tokens.get(model, 0.03)
        return (token_count / 1000) * rate
    
    def analyze_token_distribution(self, chunks: List[Dict]) -> Dict:
        """Analyze token distribution across chunks"""
        token_counts = [chunk.get('token_count', 0) for chunk in chunks]
        
        if not token_counts:
            return {'error': 'No chunks provided'}
        
        return {
            'total_chunks': len(chunks),
            'total_tokens': sum(token_counts),
            'average_tokens': sum(token_counts) / len(token_counts),
            'min_tokens': min(token_counts),
            'max_tokens': max(token_counts),
            'token_distribution': {
                'small_chunks': len([t for t in token_counts if t < 1000]),
                'medium_chunks': len([t for t in token_counts if 1000 <= t < 3000]),
                'large_chunks': len([t for t in token_counts if t >= 3000])
            }
        }

class TextProcessor:
    def __init__(self):
        pass
    
    def clean_text(self, text: str) -> str:
        """Clean and normalize text content"""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\-\.\,\;\:\!\?$$$$$$$$\{\}\"\'\/\$$', '', text)
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        return text.strip()
    
    def extract_code_blocks(self, text: str) -> List[Dict]:
        """Extract code blocks from text"""
        code_blocks = []
        
        pattern = r'```(\w*)\n(.*?)```
        matches = re.finditer(pattern, text, re.DOTALL)
        
        for match in matches:
            language = match.group(1) or 'text'
            code = match.group(2)
            code_blocks.append({
                'language': language,
                'code': code,
                'start_pos': match.start(),
                'end_pos': match.end()
            })
        
        return code_blocks
    
    def extract_headings(self, text: str) -> List[Dict]:
        """Extract headings from markdown text"""
        headings = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                headings.append({
                    'level': level,
                    'title': title,
                    'line_number': i + 1
                })
        
        return headings
    
    def create_table_of_contents(self, headings: List[Dict]) -> str:
        """Create table of contents from headings"""
        toc = "# Table of Contents\n\n"
        
        for heading in headings:
            indent = "  " * (heading['level'] - 1)
            title = heading['title']
            anchor = title.lower().replace(' ', '-').replace('.', '')
            toc += f"{indent}- [{title}](#{anchor})\n"
        
        return toc

class ValidationHelpers:
    def __init__(self):
        pass
    
    def validate_file_path(self, file_path: str) -> bool:
        """Validate if file path is safe and accessible"""
        try:
            if '..' in file_path or file_path.startswith('/'):
                return False
            
            if len(file_path) > 255:
                return False
            
            return True
        except:
            return False
    
    def validate_url(self, url: str) -> bool:
        """Validate GitLab URL format"""
        url_pattern = r'^https?://[^\s/$.?#].[^\s]*$'
        return bool(re.match(url_pattern, url))
    
    def validate_json_structure(self, json_data: Dict, required_keys: List[str]) -> Tuple[bool, List[str]]:
        """Validate JSON structure has required keys"""
        missing_keys = []
        
        for key in required_keys:
            if key not in json_data:
                missing_keys.append(key)
        
        return len(missing_keys) == 0, missing_keys
    
    def validate_template_structure(self, template_data: Dict) -> Tuple[bool, str]:
        """Validate template structure is complete"""
        required_sections = ['document_info', 'sections', 'formatting_rules']
        
        is_valid, missing = self.validate_json_structure(template_data, required_sections)
        
        if not is_valid:
            return False, f"Missing required sections: {missing}"
        
        sections = template_data.get('sections', [])
        for i, section in enumerate(sections):
            if 'name' not in section:
                return False, f"Section {i} missing 'name' field"
            if 'required' not in section:
                return False, f"Section {i} missing 'required' field"
        
        return True, "Template structure is valid"

class MetricsCalculator:
    def __init__(self):
        pass
    
    def calculate_coverage_percentage(self, processed_files: int, total_files: int) -> float:
        """Calculate file coverage percentage"""
        if total_files == 0:
            return 0.0
        return (processed_files / total_files) * 100
    
    def calculate_weighted_score(self, scores: Dict[str, float], weights: Dict[str, float]) -> float:
        """Calculate weighted average score"""
        if not scores or not weights:
            return 0.0
        
        total_weight = sum(weights.values())
        if total_weight == 0:
            return 0.0
        
        weighted_sum = sum(scores.get(key, 0) * weight for key, weight in weights.items())
        return weighted_sum / total_weight
    
    def calculate_quality_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    def generate_metrics_summary(self, metrics: Dict) -> str:
        """Generate human-readable metrics summary"""
        summary = "## Quality Metrics Summary\n\n"
        
        for metric_name, value in metrics.items():
            if isinstance(value, float):
                percentage = value * 100
                grade = self.calculate_quality_grade(value)
                summary += f"- **{metric_name.replace('_', ' ').title()}**: {percentage:.1f}% (Grade: {grade})\n"
            elif isinstance(value, dict):
                summary += f"- **{metric_name.replace('_', ' ').title()}**: {len(value)} items\n"
        
        return summary

class SourceCodeAnalyzer:
    def __init__(self):
        pass
    
    def analyze_python_file(self, file_content: str) -> Dict:
        """Analyze Python file and extract structure"""
        try:
            tree = ast.parse(file_content)
            
            analysis = {
                'functions': [],
                'classes': [],
                'imports': [],
                'variables': [],
                'docstrings': []
            }
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    analysis['functions'].append({
                        'name': node.name,
                        'args': [arg.arg for arg in node.args.args],
                        'line_number': node.lineno,
                        'docstring': ast.get_docstring(node)
                    })
                
                elif isinstance(node, ast.ClassDef):
                    analysis['classes'].append({
                        'name': node.name,
                        'line_number': node.lineno,
                        'docstring': ast.get_docstring(node),
                        'methods': [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                    })
                
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        analysis['imports'].append(alias.name)
                
                elif isinstance(node, ast.ImportFrom):
                    module = node.module or ''
                    for alias in node.names:
                        analysis['imports'].append(f"{module}.{alias.name}")
            
            return analysis
            
        except SyntaxError as e:
            return {'error': f"Syntax error in Python file: {str(e)}"}
        except Exception as e:
            return {'error': f"Error analyzing Python file: {str(e)}"}
    
    def analyze_javascript_file(self, file_content: str) -> Dict:
        """Basic analysis of JavaScript file"""
        analysis = {
            'functions': [],
            'classes': [],
            'imports': [],
            'exports': []
        }
        
        function_pattern = r'function\s+(\w+)\s*$$'
        class_pattern = r'class\s+(\w+)'
        import_pattern = r'import.*from\s+[\'"]([^\'"]+)[\'"]'
        export_pattern = r'export\s+(?:default\s+)?(?:function\s+)?(\w+)'
        
        analysis['functions'] = re.findall(function_pattern, file_content)
        analysis['classes'] = re.findall(class_pattern, file_content)
        analysis['imports'] = re.findall(import_pattern, file_content)
        analysis['exports'] = re.findall(export_pattern, file_content)
        
        return analysis
    
    def analyze_file_by_extension(self, file_path: str, file_content: str) -> Dict:
        """Analyze file based on its extension"""
        extension = Path(file_path).suffix.lower()
        
        if extension == '.py':
            return self.analyze_python_file(file_content)
        elif extension in ['.js', '.jsx', '.ts', '.tsx']:
            return self.analyze_javascript_file(file_content)
        else:
            return {
                'type': 'unsupported',
                'extension': extension,
                'line_count': len(file_content.split('\n')),
                'char_count': len(file_content)
            }

#main.py

import tkinter as tk
from tkinter import messagebox, scrolledtext, filedialog, ttk
import threading
import os
from datetime import datetime
from core import DocumentationGenerator
from evaluation import DocumentationEvaluator
from export import BatchExporter
from templates import load_template_structure
from coverage_tracker import FilesCoverageTracker
from utils import MetricsCalculator

class App(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("GitLab AI Documentation Generator with Quality Metrics")
        self.geometry("1200x900")
        self.resizable(True, True)
        
        self.doc_generator = None
        self.coverage_tracker = FilesCoverageTracker()
        self.evaluator = DocumentationEvaluator()
        self.exporter = BatchExporter()
        self.metrics_calculator = MetricsCalculator()
        self.template_structure = None
        self.generated_documentation = ""
        
        self.setup_ui()
        self.load_templates()
        
    def setup_ui(self):
        # Create notebook for tabs
        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Main generation tab
        self.setup_main_tab()
        
        # Quality metrics tab
        self.setup_metrics_tab()
        
        # Coverage analysis tab
        self.setup_coverage_tab()
        
        # Export options tab
        self.setup_export_tab()
        
    def setup_main_tab(self):
        """Setup main documentation generation tab"""
        main_frame = ttk.Frame(self.notebook)
        self.notebook.add(main_frame, text="Documentation Generation")
        
        # Title
        title_label = tk.Label(main_frame, text="GitLab AI Documentation Generator", 
                              font=("Arial", 16, "bold"))
        title_label.pack(pady=(10, 20))
        
        # URL input section
        url_frame = tk.Frame(main_frame)
        url_frame.pack(fill=tk.X, padx=20, pady=(0, 10))
        
        self.url_label = tk.Label(url_frame, text="GitLab Repository URL:", font=("Arial", 10))
        self.url_label.pack(anchor=tk.W)
        
        self.url_entry = tk.Entry(url_frame, width=80, font=("Arial", 10))
        self.url_entry.pack(fill=tk.X, pady=(5, 0))
        
        # Template selection
        template_frame = tk.Frame(main_frame)
        template_frame.pack(fill=tk.X, padx=20, pady=(5, 10))
        
        tk.Label(template_frame, text="Template:", font=("Arial", 10)).pack(anchor=tk.W)
        self.template_var = tk.StringVar()
        self.template_combo = ttk.Combobox(template_frame, textvariable=self.template_var, 
                                         values=["Default Template"], state="readonly")
        self.template_combo.pack(fill=tk.X, pady=(5, 0))
        
        # SSL Configuration
        ssl_frame = tk.Frame(main_frame)
        ssl_frame.pack(fill=tk.X, padx=20, pady=(5, 10))
        
        self.ssl_var = tk.BooleanVar(value=True)
        self.ssl_checkbox = tk.Checkbutton(ssl_frame, text="Verify SSL Certificates", 
                                         variable=self.ssl_var, font=("Arial", 9))
        self.ssl_checkbox.pack(anchor=tk.W)
        
        # Buttons frame
        button_frame = tk.Frame(main_frame)
        button_frame.pack(fill=tk.X, padx=20, pady=10)
        
        self.generate_button = tk.Button(button_frame, text="Generate Documentation", 
                                       command=self.start_generation, bg="#4CAF50", 
                                       fg="white", font=("Arial", 10, "bold"))
        self.generate_button.pack(side=tk.LEFT, padx=(0, 10))
        
        self.analyze_button = tk.Button(button_frame, text="Analyze Quality", 
                                      command=self.analyze_quality, bg="#FF9800", 
                                      fg="white", font=("Arial", 10))
        self.analyze_button.pack(side=tk.LEFT, padx=(0, 10))
        
        self.clear_button = tk.Button(button_frame, text="Clear Output", 
                                    command=self.clear_output, bg="#f44336", 
                                    fg="white", font=("Arial", 10))
        self.clear_button.pack(side=tk.LEFT)
        
        # Status section
        status_frame = tk.Frame(main_frame)
        status_frame.pack(fill=tk.X, padx=20, pady=(10, 5))
        
        self.status_label = tk.Label(status_frame, text="Status: Ready", 
                                   font=("Arial", 10), fg="green")
        self.status_label.pack(anchor=tk.W)
        
        self.progress_var = tk.StringVar()
        self.progress_label = tk.Label(status_frame, textvariable=self.progress_var, 
                                     font=("Arial", 9), fg="blue")
        self.progress_label.pack(anchor=tk.W)
        
        # Progress bar
        self.progress_bar = ttk.Progressbar(status_frame, mode='indeterminate')
        self.progress_bar.pack(fill=tk.X, pady=(5, 0))
        
        # Documentation output
        output_label = tk.Label(main_frame, text="Generated Documentation:", 
                              font=("Arial", 10, "bold"))
        output_label.pack(anchor=tk.W, padx=20, pady=(20, 5))
        
        self.text_area = scrolledtext.ScrolledText(main_frame, wrap=tk.WORD, 
                                                 font=("Consolas", 9), height=20)
        self.text_area.pack(fill=tk.BOTH, expand=True, padx=20, pady=(0, 10))
    
    def setup_metrics_tab(self):
        """Setup quality metrics tab"""
        metrics_frame = ttk.Frame(self.notebook)
        self.notebook.add(metrics_frame, text="Quality Metrics")
        
        # Metrics display
        tk.Label(metrics_frame, text="Documentation Quality Metrics", 
                font=("Arial", 14, "bold")).pack(pady=10)
        
        # Metrics text area
        self.metrics_text = scrolledtext.ScrolledText(metrics_frame, wrap=tk.WORD, 
                                                    font=("Consolas", 9), height=25)
        self.metrics_text.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)
        
        # Metrics buttons
        metrics_button_frame = tk.Frame(metrics_frame)
        metrics_button_frame.pack(fill=tk.X, padx=20, pady=10)
        
        tk.Button(metrics_button_frame, text="Export Metrics", 
                 command=self.export_metrics, bg="#2196F3", fg="white").pack(side=tk.LEFT)
    
    def setup_coverage_tab(self):
        """Setup coverage analysis tab"""
        coverage_frame = ttk.Frame(self.notebook)
        self.notebook.add(coverage_frame, text="Coverage Analysis")
        
        tk.Label(coverage_frame, text="File Coverage Analysis", 
                font=("Arial", 14, "bold")).pack(pady=10)
        
        # Coverage metrics display
        self.coverage_text = scrolledtext.ScrolledText(coverage_frame, wrap=tk.WORD, 
                                                     font=("Consolas", 9), height=25)
        self.coverage_text.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)
    
    def setup_export_tab(self):
        """Setup export options tab"""
        export_frame = ttk.Frame(self.notebook)
        self.notebook.add(export_frame, text="Export Options")
        
        tk.Label(export_frame, text="Export Documentation", 
                font=("Arial", 14, "bold")).pack(pady=10)
        
        # Export format selection
        format_frame = tk.Frame(export_frame)
        format_frame.pack(fill=tk.X, padx=20, pady=10)
        
        tk.Label(format_frame, text="Export Format:", font=("Arial", 10)).pack(anchor=tk.W)
        
        self.export_format = tk.StringVar(value="pdf")
        formats = [("PDF", "pdf"), ("DOCX", "docx"), ("Markdown", "md"), ("All Formats", "all")]
        
        for text, value in formats:
            tk.Radiobutton(format_frame, text=text, variable=self.export_format, 
                          value=value, font=("Arial", 9)).pack(anchor=tk.W)
        
        # Export buttons
        export_button_frame = tk.Frame(export_frame)
        export_button_frame.pack(fill=tk.X, padx=20, pady=20)
        
        self.export_button = tk.Button(export_button_frame, text="Export Documentation", 
                                     command=self.export_documentation, bg="#4CAF50", 
                                     fg="white", font=("Arial", 10, "bold"), state=tk.DISABLED)
        self.export_button.pack(side=tk.LEFT, padx=(0, 10))
        
        # Export status
        self.export_status = tk.Label(export_frame, text="", font=("Arial", 9))
        self.export_status.pack(padx=20, pady=10)
    
    def load_templates(self):
        """Load available templates"""
        try:
            self.template_structure = load_template_structure()
            if self.template_structure:
                template_name = self.template_structure.get('document_info', {}).get('title', 'Loaded Template')
                self.template_combo['values'] = [template_name]
                self.template_combo.current(0)
            else:
                self.template_combo['values'] = ["Default Template"]
                self.template_combo.current(0)
        except Exception as e:
            print(f"Error loading templates: {str(e)}")
            self.template_combo['values'] = ["Default Template"]
            self.template_combo.current(0)
    
    def start_generation(self):
        """Start documentation generation"""
        repo_url = self.url_entry.get().strip()
        if not repo_url:
            messagebox.showerror("Error", "Please enter a GitLab repository URL.")
            return
        
        if not (repo_url.startswith('http://') or repo_url.startswith('https://')):
            messagebox.showerror("Error", "Please enter a valid URL starting with http:// or https://")
            return
            
        self.generate_button.config(state=tk.DISABLED)
        self.analyze_button.config(state=tk.DISABLED)
        self.export_button.config(state=tk.DISABLED)
        self.text_area.delete(1.0, tk.END)
        self.progress_bar.start()
        
        # Initialize doc generator with SSL setting
        verify_ssl = self.ssl_var.get()
        self.doc_generator = DocumentationGenerator(verify_ssl=verify_ssl)
        
        # Start generation in separate thread
        thread = threading.Thread(target=self.generate_docs, args=(repo_url,))
        thread.daemon = True
        thread.start()
    
    def generate_docs(self, repo_url):
        """Generate documentation in background thread"""
        try:
            def update_status(message):
                self.after(0, lambda: self.status_label.config(text=f"Status: {message}"))
                self.after(0, lambda: self.progress_var.set(f"● {message}"))
            
            update_status("Starting documentation generation...")
            
            # Generate documentation with template structure
            documentation = self.doc_generator.generate_documentation(
                repo_url, self.template_structure, update_status
            )
            
            self.generated_documentation = documentation
            
            # Get processing stats for coverage analysis
            processing_stats = self.doc_generator.get_processing_stats()
            
            # Update UI
            self.after(0, lambda: self.text_area.insert(tk.END, documentation))
            self.after(0, lambda: self.status_label.config(text="Status: Documentation generated successfully!", fg="green"))
            self.after(0, lambda: self.progress_var.set("✓ Complete"))
            self.after(0, lambda: self.export_button.config(state=tk.NORMAL))
            self.after(0, lambda: self.analyze_button.config(state=tk.NORMAL))
            
            # Update coverage tab
            self.after(0, lambda: self.update_coverage_display(processing_stats))
            
        except Exception as e:
            error_msg = str(e)
            self.after(0, lambda: messagebox.showerror("Error", f"Failed to generate documentation:\n\n{error_msg}"))
            self.after(0, lambda: self.status_label.config(text="Status: Error occurred", fg="red"))
            self.after(0, lambda: self.progress_var.set("✗ Failed"))
            
        finally:
            self.after(0, lambda: self.generate_button.config(state=tk.NORMAL))
            self.after(0, lambda: self.progress_bar.stop())
    
    def analyze_quality(self):
        """Analyze documentation quality"""
        if not self.generated_documentation:
            messagebox.showwarning("Warning", "No documentation to analyze. Generate documentation first.")
            return
        
        try:
            # Prepare reference data
            reference_data = {
                'template_structure': self.template_structure,
                'source_code_analysis': {},  # Would be populated from actual analysis
                'reference_docs': []  # Optional reference documentation
            }
            
            # Perform evaluation
            metrics = self.evaluator.comprehensive_evaluation(self.generated_documentation, reference_data)
            
            # Generate report
            report = self.evaluator.generate_evaluation_report()
            
            # Display in metrics tab
            self.metrics_text.delete(1.0, tk.END)
            self.metrics_text.insert(tk.END, report)
            
            # Switch to metrics tab
            self.notebook.select(1)
            
            messagebox.showinfo("Analysis Complete", "Quality analysis completed. Check the Quality Metrics tab.")
            
        except Exception as e:
            messagebox.showerror("Error", f"Failed to analyze quality:\n\n{str(e)}")
    
    def update_coverage_display(self, processing_stats):
        """Update coverage analysis display"""
        try:
            coverage_report = f"""
# File Coverage Analysis

## Processing Summary
- **Total Files in Repository**: {processing_stats['total_files']}
- **Files Processed**: {processing_stats['processed_files']}
- **Files Skipped**: {processing_stats['skipped_files']}
- **Coverage Percentage**: {processing_stats['coverage_percentage']:.1f}%

## Token Analysis
- **Total Tokens Processed**: {processing_stats['total_tokens']:,}
- **Chunks Created**: {processing_stats['chunks_created']}
- **Average Tokens per Chunk**: {processing_stats['total_tokens'] // max(processing_stats['chunks_created'], 1):,}

## File Manifest
### Processed Files:
"""
            
            for file_path in processing_stats['file_manifest']['processed_files'][:20]:
                coverage_report += f"- ✅ {file_path}\n"
            
            if len(processing_stats['file_manifest']['processed_files']) > 20:
                remaining = len(processing_stats['file_manifest']['processed_files']) - 20
                coverage_report += f"- ... and {remaining} more files\n"
            
            # Continuing from where it was cut off...
            if processing_stats['file_manifest']['skipped_files']:
                coverage_report += "\n### Skipped Files:\n"
                for file_path in processing_stats['file_manifest']['skipped_files'][:10]:
                    coverage_report += f"- ❌ {file_path}\n"
            
            coverage_report += f"\n## Quality Assessment\n"
            coverage_report += f"- **Coverage Status**: {'✅ PASS' if processing_stats['coverage_percentage'] >= 90 else '⚠️ NEEDS IMPROVEMENT'}\n"
            coverage_report += f"- **Token Efficiency**: {'✅ OPTIMAL' if processing_stats['total_tokens'] < 100000 else '⚠️ HIGH USAGE'}\n"
            
            self.coverage_text.delete(1.0, tk.END)
            self.coverage_text.insert(tk.END, coverage_report)
            
        except Exception as e:
            self.coverage_text.delete(1.0, tk.END)
            self.coverage_text.insert(tk.END, f"Error generating coverage report: {str(e)}")
    
    def export_documentation(self):
        """Export documentation to selected format"""
        if not self.generated_documentation:
            messagebox.showwarning("Warning", "No documentation to export. Generate documentation first.")
            return
        
        try:
            format_type = self.export_format.get()
            
            template_info = {
                'title': 'GitLab Repository Documentation',
                'repo_url': self.url_entry.get(),
                'generated_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'total_files': getattr(self.doc_generator, 'processing_stats', {}).get('total_files', 0),
                'coverage': getattr(self.doc_generator, 'processing_stats', {}).get('coverage_percentage', 0)
            }
            
            if format_type == "all":
                file_path = filedialog.asksaveasfilename(
                    defaultextension="",
                    filetypes=[("All files", "*.*")],
                    title="Choose base filename for exports"
                )
                
                if file_path:
                    base_name = os.path.splitext(file_path)[0]
                    results = self.exporter.export_all_formats(
                        self.generated_documentation, base_name, template_info
                    )
                    
                    success_count = sum(1 for success in results.values() if success)
                    self.export_status.config(
                        text=f"Exported {success_count}/{len(results)} formats successfully",
                        fg="green" if success_count == len(results) else "orange"
                    )
                    
                    messagebox.showinfo("Export Complete", 
                                      f"Documentation exported to {success_count} formats:\n" + 
                                      "\n".join([f"- {fmt}: {'✅' if success else '❌'}" 
                                               for fmt, success in results.items()]))
            else:
                file_extensions = {"pdf": ".pdf", "docx": ".docx", "md": ".md"}
                file_types = {
                    "pdf": [("PDF files", "*.pdf")],
                    "docx": [("Word documents", "*.docx")],
                    "md": [("Markdown files", "*.md")]
                }
                
                file_path = filedialog.asksaveasfilename(
                    defaultextension=file_extensions[format_type],
                    filetypes=file_types[format_type],
                    title=f"Save as {format_type.upper()}"
                )
                
                if file_path:
                    success = self.exporter.export_single_format(
                        self.generated_documentation, file_path, format_type, template_info
                    )
                    
                    if success:
                        self.export_status.config(text=f"Successfully exported to {file_path}", fg="green")
                        messagebox.showinfo("Export Complete", f"Documentation exported to:\n{file_path}")
                    else:
                        self.export_status.config(text="Export failed", fg="red")
                        messagebox.showerror("Export Failed", "Failed to export documentation")
                        
        except Exception as e:
            self.export_status.config(text=f"Export error: {str(e)}", fg="red")
            messagebox.showerror("Export Error", f"Failed to export documentation:\n\n{str(e)}")
    
    def export_metrics(self):
        """Export quality metrics to JSON file"""
        if not hasattr(self.evaluator, 'metrics') or not self.evaluator.metrics:
            messagebox.showwarning("Warning", "No metrics to export. Run quality analysis first.")
            return
        
        try:
            file_path = filedialog.asksaveasfilename(
                defaultextension=".json",
                filetypes=[("JSON files", "*.json")],
                title="Save metrics as JSON"
            )
            
            if file_path:
                self.evaluator.export_metrics(file_path)
                messagebox.showinfo("Export Complete", f"Metrics exported to:\n{file_path}")
                
        except Exception as e:
            messagebox.showerror("Export Error", f"Failed to export metrics:\n\n{str(e)}")
    
    def clear_output(self):
        """Clear all output areas"""
        self.text_area.delete(1.0, tk.END)
        self.metrics_text.delete(1.0, tk.END)
        self.coverage_text.delete(1.0, tk.END)
        self.status_label.config(text="Status: Ready", fg="green")
        self.progress_var.set("")
        self.export_status.config(text="")
        self.export_button.config(state=tk.DISABLED)
        self.analyze_button.config(state=tk.DISABLED)
        self.generated_documentation = ""

if __name__ == '__main__':
    app = App()
    app.mainloop()


#tests/test_file_coverage.py


import unittest
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from coverage_tracker import FilesCoverageTracker
from core import DocumentationGenerator, CodeProcessor
from utils import FileTracker

class TestFileCoverage(unittest.TestCase):
    """Essential tests to ensure no files are missed during processing"""
    
    def setUp(self):
        self.coverage_tracker = FilesCoverageTracker()
        self.code_processor = CodeProcessor()
        self.file_tracker = FileTracker()
        
        self.sample_repo_files = [
            {'path': 'main.py', 'size': 5000, 'type': 'blob'},
            {'path': 'core.py', 'size': 8000, 'type': 'blob'},
            {'path': 'config.py', 'size': 1500, 'type': 'blob'},
            {'path': 'utils.py', 'size': 3000, 'type': 'blob'},
            {'path': 'README.md', 'size': 800, 'type': 'blob'},
            {'path': 'requirements.txt', 'size': 200, 'type': 'blob'},
            {'path': 'tests/test_main.py', 'size': 2000, 'type': 'blob'},
            {'path': 'docs/api.md', 'size': 1200, 'type': 'blob'},
            {'path': '.gitignore', 'size': 100, 'type': 'blob'},
            {'path': 'node_modules/package.json', 'size': 500, 'type': 'blob'},
        ]
    
    def test_complete_file_inventory(self):
        """CRITICAL: Test that all repository files are cataloged"""
        manifest = self.coverage_tracker.create_file_manifest(self.sample_repo_files)
        
        self.assertEqual(manifest['total_count'], len(self.sample_repo_files))
        
        critical_files = manifest['critical_files']
        self.assertIn('main.py', critical_files)
        self.assertIn('README.md', critical_files)
        self.assertIn('requirements.txt', critical_files)
        
        print(f"✅ File inventory test passed: {manifest['total_count']} files cataloged")
    
    def test_file_filtering_accuracy(self):
        """CRITICAL: Test that file filtering doesn't miss important files"""
        filtered_files = self.code_processor.filter_files(self.sample_repo_files)
        filtered_paths = [f['path'] for f in filtered_files]
        
        critical_files = ['main.py', 'core.py', 'config.py', 'utils.py', 'README.md']
        for critical_file in critical_files:
            self.assertIn(critical_file, filtered_paths, 
                         f"Critical file {critical_file} was filtered out!")
        
        self.assertNotIn('node_modules/package.json', filtered_paths)
        
        print(f"✅ File filtering test passed: {len(filtered_files)} files selected")
    
    def test_coverage_percentage_calculation(self):
        """CRITICAL: Test coverage percentage accuracy"""
        self.coverage_tracker.total_files = 10
        self.coverage_tracker.processed_files = 8
        
        metrics = self.coverage_tracker.calculate_coverage_metrics()
        
        expected_coverage = 80.0
        self.assertEqual(metrics['coverage_percentage'], expected_coverage)
        self.assertEqual(metrics['file_coverage'], 0.8)
        
        print(f"✅ Coverage calculation test passed: {metrics['coverage_percentage']}%")
    
    def test_missing_file_detection(self):
        """CRITICAL: Test detection of files that should be processed but aren't"""
        all_files = ['main.py', 'config.py', 'utils.py', 'README.md']
        processed_files = ['main.py', 'config.py']
        
        missing_files = set(all_files) - set(processed_files)
        
        self.assertEqual(len(missing_files), 2)
        self.assertIn('utils.py', missing_files)
        self.assertIn('README.md', missing_files)
        
        print(f"✅ Missing file detection test passed: {len(missing_files)} missing files detected")
    
    def test_token_counting_accuracy(self):
        """CRITICAL: Test that token counting is accurate"""
        sample_code = """
def hello_world():
    print("Hello, World!")
    return True

class TestClass:
    def __init__(self):
        self.value = 42
"""
        
        token_count = self.code_processor.count_tokens(sample_code)
        
        self.assertGreater(token_count, 0, "Token count should be greater than 0")
        self.assertLess(token_count, 1000, "Token count seems too high for small sample")
        
        print(f"✅ Token counting test passed: {token_count} tokens counted")

if __name__ == '__main__':
    unittest.main()

#tests/test_accuracy_validation.py

import unittest
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaluation import DocumentationEvaluator
from templates import TemplateValidator
import numpy as np

class TestAccuracyValidation(unittest.TestCase):
    """Essential tests to ensure documentation accuracy and quality metrics"""

    def setUp(self):
        self.evaluator = DocumentationEvaluator()
        self.template_validator = TemplateValidator()

        # Sample template structure for testing
        self.sample_template = {
            'sections': [
                {'name': 'Project Overview', 'required': True, 'order': 1},
                {'name': 'Installation Guide', 'required': True, 'order': 2},
                {'name': 'API Reference', 'required': True, 'order': 3},
                {'name': 'Usage Examples', 'required': False, 'order': 4}
            ]
        }

        # Sample generated documentation
        self.sample_generated_doc = """
# Project Overview
This is a sample project that demonstrates functionality.

# Installation Guide
1. Clone the repository
2. Install dependencies
3. Run the application

# API Reference
## Functions
- get_data(): Returns data from source
- process_data(): Processes the input data

# Usage Examples
data = get_data()
result = process_data(data)

text
"""

        # Sample source code analysis
        self.sample_code_analysis = {
            'functions': ['get_data', 'process_data', 'save_data'],
            'classes': ['DataProcessor', 'FileHandler'],
            'dependencies': ['requests', 'pandas', 'numpy']
        }

        # Sample reference documentation
        self.sample_reference_docs = [
            "This is a sample project for testing documentation quality.",
            "Installation involves cloning and running setup commands.",
            "The API provides functions for data processing and file handling."
        ]

    def test_template_compliance_accuracy(self):
        """CRITICAL: Test template compliance checking accuracy"""
        compliance = self.evaluator.evaluate_template_compliance(
            self.sample_generated_doc, self.sample_template
        )
        self.assertGreater(compliance['f1_score'], 0.7)
        self.assertLessEqual(compliance['f1_score'], 1.0)
        self.assertEqual(compliance['sections_found'], 4)
        self.assertEqual(compliance['sections_required'], 3)
        section_details = compliance['section_details']
        self.assertTrue(section_details['Project Overview']['found'])
        self.assertTrue(section_details['Installation Guide']['found'])
        self.assertTrue(section_details['API Reference']['found'])
        print(f"✅ Template compliance test passed: F1 = {compliance['f1_score']:.3f}")

    def test_confusion_matrix_calculation(self):
        """CRITICAL: Test confusion matrix generation"""
        compliance = self.evaluator.evaluate_template_compliance(
            self.sample_generated_doc, self.sample_template
        )
        cm = compliance['confusion_matrix']
        self.assertIsInstance(cm, list)
        self.assertEqual(len(cm), 2)
        self.assertEqual(len(cm[0]), 2)
        for row in cm:
            for value in row:
                self.assertGreaterEqual(value, 0)
                self.assertIsInstance(value, (int, np.integer))
        print(f"✅ Confusion matrix test passed: {cm}")

    def test_f1_score_calculation(self):
        """CRITICAL: Test F1 score computation accuracy"""
        actual_labels = [1, 1, 1, 0]
        predicted_labels = [1, 1, 1, 1]
        from sklearn.metrics import f1_score
        expected_f1 = f1_score(actual_labels, predicted_labels, average='weighted')
        compliance = self.evaluator.evaluate_template_compliance(
            self.sample_generated_doc, self.sample_template
        )
        self.assertIsInstance(compliance['f1_score'], float)
        self.assertGreaterEqual(compliance['f1_score'], 0.0)
        self.assertLessEqual(compliance['f1_score'], 1.0)
        self.assertIn('precision', compliance)
        self.assertIn('recall', compliance)
        print(f"✅ F1 score calculation test passed: {compliance['f1_score']:.3f}")

    def test_code_accuracy_evaluation(self):
        """CRITICAL: Test code accuracy assessment"""
        code_metrics = self.evaluator.evaluate_code_accuracy(
            self.sample_generated_doc, self.sample_code_analysis
        )
        self.assertIn('f1_score', code_metrics)
        self.assertIn('function_accuracy', code_metrics)
        self.assertIn('class_accuracy', code_metrics)
        self.assertIn('dependency_accuracy', code_metrics)
        self.assertGreaterEqual(code_metrics['f1_score'], 0.0)
        self.assertLessEqual(code_metrics['f1_score'], 1.0)
        self.assertGreaterEqual(code_metrics['function_accuracy'], 0.0)
        self.assertLessEqual(code_metrics['function_accuracy'], 1.0)
        details = code_metrics['details']
        self.assertIn('functions', details)
        self.assertIn('classes', details)
        self.assertIn('dependencies', details)
        print(f"✅ Code accuracy test passed: F1 = {code_metrics['f1_score']:.3f}")

    def test_content_quality_assessment(self):
        """CRITICAL: Test content quality evaluation"""
        content_metrics = self.evaluator.evaluate_content_quality(
            self.sample_generated_doc, self.sample_reference_docs
        )
        self.assertIn('quality_score', content_metrics)
        self.assertIn('bleu_score', content_metrics)
        self.assertIn('rouge_scores', content_metrics)
        self.assertIn('readability_score', content_metrics)
        self.assertIn('completeness_score', content_metrics)
        self.assertGreaterEqual(content_metrics['quality_score'], 0.0)
        self.assertLessEqual(content_metrics['quality_score'], 1.0)
        self.assertGreaterEqual(content_metrics['bleu_score'], 0.0)
        self.assertLessEqual(content_metrics['bleu_score'], 1.0)
        self.assertIsInstance(content_metrics['rouge_scores'], dict)
        print(f"✅ Content quality test passed: Score = {content_metrics['quality_score']:.3f}")

    def test_overall_quality_calculation(self):
        """CRITICAL: Test overall quality score computation"""
        template_metrics = {'f1_score': 0.9}
        code_metrics = {'f1_score': 0.8}
        content_metrics = {'quality_score': 0.85}
        overall_score = self.evaluator.calculate_overall_quality(
            template_metrics, code_metrics, content_metrics
        )
        self.assertIsInstance(overall_score, float)
        self.assertGreaterEqual(overall_score, 0.0)
        self.assertLessEqual(overall_score, 1.0)
        expected_score = (0.9 * 0.3) + (0.8 * 0.4) + (0.85 * 0.3)
        self.assertAlmostEqual(overall_score, expected_score, places=2)
        print(f"✅ Overall quality test passed: Score = {overall_score:.3f}")

    def test_comprehensive_evaluation(self):
        """CRITICAL: Test complete evaluation pipeline"""
        reference_data = {
            'template_structure': self.sample_template,
            'source_code_analysis': self.sample_code_analysis,
            'reference_docs': self.sample_reference_docs
        }
        metrics = self.evaluator.comprehensive_evaluation(
            self.sample_generated_doc, reference_data
        )
        required_metrics = [
            'template_compliance_f1',
            'code_accuracy_f1',
            'content_quality_score',
            'bleu_score',
            'rouge_scores',
            'overall_quality',
            'detailed_metrics'
        ]
        for metric in required_metrics:
            self.assertIn(metric, metrics)
        detailed = metrics['detailed_metrics']
        self.assertIn('template', detailed)
        self.assertIn('code', detailed)
        self.assertIn('content', detailed)
        print(f"✅ Comprehensive evaluation test passed: Overall = {metrics['overall_quality']:.3f}")

    def test_evaluation_report_generation(self):
        """CRITICAL: Test evaluation report generation"""
        reference_data = {
            'template_structure': self.sample_template,
            'source_code_analysis': self.sample_code_analysis,
            'reference_docs': self.sample_reference_docs
        }
        self.evaluator.comprehensive_evaluation(self.sample_generated_doc, reference_data)
        report = self.evaluator.generate_evaluation_report()
        self.assertIsInstance(report, str)
        self.assertGreater(len(report), 100)
        self.assertIn("Documentation Quality Evaluation Report", report)
        self.assertIn("Overall Quality Score", report)
        self.assertIn("Template Compliance", report)
        self.assertIn("Code Accuracy", report)
        self.assertIn("Content Quality", report)
        print(f"✅ Evaluation report test passed: {len(report)} characters generated")

    def test_metrics_export(self):
        """CRITICAL: Test metrics export functionality"""
        import tempfile
        import json
        reference_data = {
            'template_structure': self.sample_template,
            'source_code_analysis': self.sample_code_analysis,
            'reference_docs': self.sample_reference_docs
        }
        self.evaluator.comprehensive_evaluation(self.sample_generated_doc, reference_data)
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as temp_file:
            temp_path = temp_file.name
        try:
            self.evaluator.export_metrics(temp_path)
            self.assertTrue(os.path.exists(temp_path))
            with open(temp_path, 'r') as f:
                exported_data = json.load(f)
            self.assertIsInstance(exported_data, dict)
            self.assertIn('overall_quality', exported_data)
            print(f"✅ Metrics export test passed: {len(exported_data)} metrics exported")
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)

if __name__ == '__main__':
    unittest.main(verbosity=2)



#export.py

import os
import markdown
from weasyprint import HTML, CSS
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from docx import Document
from docx.shared import Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.style import WD_STYLE_TYPE
import re
from typing import Dict, Optional
from pathlib import Path

class PDFExporter:
    def __init__(self):
        self.styles = getSampleStyleSheet()
        self._setup_custom_styles()
    
    def _setup_custom_styles(self):
        """Setup custom styles for PDF generation"""
        # Title style
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Title'],
            fontSize=24,
            spaceAfter=30,
            alignment=1,  # Center alignment
            textColor=colors.darkblue
        ))
        
        # Heading styles
        self.styles.add(ParagraphStyle(
            name='CustomHeading1',
            parent=self.styles['Heading1'],
            fontSize=18,
            spaceAfter=12,
            textColor=colors.darkblue,
            borderWidth=1,
            borderColor=colors.darkblue,
            borderPadding=5
        ))
        
        # Code style
        self.styles.add(ParagraphStyle(
            name='Code',
            parent=self.styles['Normal'],
            fontName='Courier',
            fontSize=10,
            leftIndent=20,
            backgroundColor=colors.lightgrey,
            borderWidth=1,
            borderColor=colors.grey,
            borderPadding=5
        ))
    
    def export_to_pdf(self, content: str, output_path: str, template_info: Optional[Dict] = None) -> bool:
        """Export markdown content to PDF"""
        try:
            # Create PDF document
            doc = SimpleDocTemplate(output_path, pagesize=A4)
            story = []
            
            # Add title page
            if template_info:
                story.extend(self._create_title_page(template_info))
            
            # Convert markdown to PDF elements
            story.extend(self._markdown_to_pdf_elements(content))
            
            # Build PDF
            doc.build(story)
            return True
            
        except Exception as e:
            print(f"Error exporting to PDF: {str(e)}")
            return False
    
    def _create_title_page(self, template_info: Dict) -> list:
        """Create title page for PDF"""
        story = []
        
        # Title
        title = template_info.get('title', 'Documentation')
        story.append(Paragraph(title, self.styles['CustomTitle']))
        story.append(Spacer(1, 0.5*inch))
        
        # Metadata table
        metadata = [
            ['Generated from:', template_info.get('repo_url', 'Unknown')],
            ['Generated on:', template_info.get('generated_date', 'Unknown')],
            ['Total files:', str(template_info.get('total_files', 0))],
            ['Coverage:', f"{template_info.get('coverage', 0):.1f}%"]
        ]
        
        table = Table(metadata, colWidths=[2*inch, 4*inch])
        table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, -1), colors.lightgrey),
            ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),
            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
            ('FONTSIZE', (0, 0), (-1, -1), 12),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 12),
            ('BACKGROUND', (0, 0), (0, -1), colors.grey),
            ('TEXTCOLOR', (0, 0), (0, -1), colors.whitesmoke),
        ]))
        
        story.append(table)
        story.append(Spacer(1, 1*inch))
        
        return story
    
    def _markdown_to_pdf_elements(self, content: str) -> list:
        """Convert markdown content to PDF elements"""
        story = []
        lines = content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
            
            # Handle headers
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                text = line.lstrip('#').strip()
                
                if level == 1:
                    story.append(Paragraph(text, self.styles['CustomHeading1']))
                elif level == 2:
                    story.append(Paragraph(text, self.styles['Heading2']))
                else:
                    story.append(Paragraph(text, self.styles['Heading3']))
                
                story.append(Spacer(1, 12))
            
            # Handle code blocks
            elif line.startswith('```
                i += 1
                code_lines = []
                while i < len(lines) and not lines[i].strip().startswith('```'):
                    code_lines.append(lines[i])
                    i += 1
                
                code_content = '\n'.join(code_lines)
                story.append(Paragraph(code_content, self.styles['Code']))
                story.append(Spacer(1, 12))
            
            # Handle regular paragraphs
            else:
                # Collect paragraph lines
                para_lines = [line]
                i += 1
                while i < len(lines) and lines[i].strip() and not lines[i].startswith('#') and not lines[i].startswith('```
                    para_lines.append(lines[i].strip())
                    i += 1
                
                para_text = ' '.join(para_lines)
                story.append(Paragraph(para_text, self.styles['Normal']))
                story.append(Spacer(1, 12))
                continue
            
            i += 1
        
        return story

class DOCXExporter:
    def __init__(self):
        self.doc = None
        
    def export_to_docx(self, content: str, output_path: str, template_info: Optional[Dict] = None) -> bool:
        """Export markdown content to DOCX"""
        try:
            self.doc = Document()
            
            # Add title page
            if template_info:
                self._create_docx_title_page(template_info)
            
            # Convert markdown to DOCX
            self._markdown_to_docx(content)
            
            # Save document
            self.doc.save(output_path)
            return True
            
        except Exception as e:
            print(f"Error exporting to DOCX: {str(e)}")
            return False
    
    def _create_docx_title_page(self, template_info: Dict):
        """Create title page for DOCX"""
        # Title
        title = self.doc.add_heading(template_info.get('title', 'Documentation'), 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # Add metadata
        self.doc.add_paragraph()
        metadata_para = self.doc.add_paragraph()
        metadata_para.add_run('Generated from: ').bold = True
        metadata_para.add_run(template_info.get('repo_url', 'Unknown'))
        
        date_para = self.doc.add_paragraph()
        date_para.add_run('Generated on: ').bold = True
        date_para.add_run(template_info.get('generated_date', 'Unknown'))
        
        files_para = self.doc.add_paragraph()
        files_para.add_run('Total files: ').bold = True
        files_para.add_run(str(template_info.get('total_files', 0)))
        
        coverage_para = self.doc.add_paragraph()
        coverage_para.add_run('Coverage: ').bold = True
        coverage_para.add_run(f"{template_info.get('coverage', 0):.1f}%")
        
        # Page break
        self.doc.add_page_break()
    
    def _markdown_to_docx(self, content: str):
        """Convert markdown content to DOCX"""
        lines = content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            if not line:
                i += 1
                continue
            
            # Handle headers
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                text = line.lstrip('#').strip()
                self.doc.add_heading(text, level)
            
            # Handle code blocks
            elif line.startswith('```'):
                i += 1
                code_lines = []
                while i < len(lines) and not lines[i].strip().startswith('```
                    code_lines.append(lines[i])
                    i += 1
                
                code_content = '\n'.join(code_lines)
                code_para = self.doc.add_paragraph(code_content)
                code_para.style = 'Intense Quote'
            
            # Handle bullet points
            elif line.startswith('- ') or line.startswith('* '):
                text = line[2:].strip()
                self.doc.add_paragraph(text, style='List Bullet')
            
            # Handle numbered lists
            elif re.match(r'^\d+\.\s', line):
                text = re.sub(r'^\d+\.\s', '', line)
                self.doc.add_paragraph(text, style='List Number')
            
            # Handle regular paragraphs
            else:
                # Collect paragraph lines
                para_lines = [line]
                i += 1
                while i < len(lines) and lines[i].strip() and not lines[i].startswith('#') and not lines[i].startswith('```'):
                    para_lines.append(lines[i].strip())
                    i += 1
                
                para_text = ' '.join(para_lines)
                self.doc.add_paragraph(para_text)
                continue
            
            i += 1

class MarkdownExporter:
    def __init__(self):
        pass
    
    def export_to_markdown(self, content: str, output_path: str, template_info: Optional[Dict] = None) -> bool:
        """Export content to markdown file with metadata"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                # Add metadata header
                if template_info:
                    f.write(self._create_markdown_metadata(template_info))
                
                # Add content
                f.write(content)
            
            return True
            
        except Exception as e:
            print(f"Error exporting to Markdown: {str(e)}")
            return False
    
    def _create_markdown_metadata(self, template_info: Dict) -> str:
        """Create metadata header for markdown"""
        metadata = f"""---
title: {template_info.get('title', 'Documentation')}
generated_from: {template_info.get('repo_url', 'Unknown')}
generated_date: {template_info.get('generated_date', 'Unknown')}
total_files: {template_info.get('total_files', 0)}
coverage: {template_info.get('coverage', 0):.1f}%
---

"""
        return metadata

class BatchExporter:
    def __init__(self):
        self.pdf_exporter = PDFExporter()
        self.docx_exporter = DOCXExporter()
        self.markdown_exporter = MarkdownExporter()
    
    def export_all_formats(self, content: str, base_filename: str, template_info: Optional[Dict] = None) -> Dict[str, bool]:
        """Export content to all supported formats"""
        results = {}
        
        # Export to PDF
        pdf_path = f"{base_filename}.pdf"
        results['pdf'] = self.pdf_exporter.export_to_pdf(content, pdf_path, template_info)
        
        # Export to DOCX
        docx_path = f"{base_filename}.docx"
        results['docx'] = self.docx_exporter.export_to_docx(content, docx_path, template_info)
        
        # Export to Markdown
        md_path = f"{base_filename}.md"
        results['markdown'] = self.markdown_exporter.export_to_markdown(content, md_path, template_info)
        
        return results
    
    def export_single_format(self, content: str, output_path: str, format_type: str, template_info: Optional[Dict] = None) -> bool:
        """Export content to a single format"""
        format_type = format_type.lower()
        
        if format_type == 'pdf':
            return self.pdf_exporter.export_to_pdf(content, output_path, template_info)
        elif format_type in ['docx', 'doc']:
            return self.docx_exporter.export_to_docx(content, output_path, template_info)
        elif format_type in ['md', 'markdown']:
            return self.markdown_exporter.export_to_markdown(content, output_path, template_info)
        else:
            print(f"Unsupported format: {format_type}")
            return False



gitlab-ai-doc-generator/
├── main.py                           # Enhanced UI with coverage dashboard
├── core.py                           # GitLab, LLaMA, processing with token tracking
├── evaluation.py                     # Confusion matrix, F1 score, coverage metrics
├── export.py                         # PDF/DOCX/MD export with template styling
├── config.py                         # Configuration with quality thresholds
├── templates.py                      # Auto template parsing and validation
├── utils.py                          # File tracking and token counting utilities
├── coverage_tracker.py              # File coverage and token limit testing
├── requirements.txt                  # All dependencies
├── .env                             # Environment variables
├── README.md                        # Project documentation
├── templates/                       # Template files directory
│   ├── documentation_template.pdf   # Your reference template (PDF)
│   ├── documentation_template.docx  # Your reference template (DOCX)
│   └── template_structure.json      # Auto-generated template structure
└── tests/                           # Minimal essential tests only
    ├── test_file_coverage.py        # Essential: File coverage validation
    └── test_accuracy_validation.py  # Essential: Accuracy and quality checks


# Core functionality
python-dotenv==1.0.0
requests==2.28.2
urllib3==1.26.18

# Template processing
PyPDF2==2.12.1
python-docx==0.8.11
pdfplumber==0.7.6

# Export capabilities
reportlab==3.6.13
markdown==3.5.2

# Evaluation metrics
nltk==3.7
scikit-learn==1.0.2
numpy==1.21.6
pandas==1.3.5
rouge-score==0.1.2

# Text processing
beautifulsoup4==4.11.2

# Testing framework
pytest==7.2.2
pytest-cov==4.0.0

# Visualization for metrics
matplotlib==3.5.3
seaborn==0.11.2

# GUI framework (built-in)
# tkinter - included with Python

# Optional: WeasyPrint (may require system dependencies)
# weasyprint==57.2
