import re
import json
import requests
import urllib3
from datetime import datetime
from config import OPENAI_API_KEY, OPENAI_BASE_URL, AI_MODEL, MAX_TOKENS, TEMPERATURE, ERROR_KEYWORDS
import logging

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIAnalyzer:
    def __init__(self, knowledge_base):
        self.knowledge_base = knowledge_base
        self.error_patterns = self._build_error_patterns()
        self.api_headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        # Fix the URL - remove duplicate /chat/completions
        base_url = OPENAI_BASE_URL.rstrip('/')
        if base_url.endswith('/chat/completions'):
            self.api_url = base_url
        else:
            self.api_url = f"{base_url}/chat/completions"
        
        # Configure session with SSL bypass
        self.session = requests.Session()
        self.session.headers.update(self.api_headers)
        self.session.verify = False  # Disable SSL verification
        
        # Test API connection
        self._test_api_connection()
    
    def _test_api_connection(self):
        """Test connection to API with SSL bypass"""
        try:
            test_payload = {
                "model": AI_MODEL,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 10
            }
            
            logger.info(f"Testing API connection to: {self.api_url}")
            
            response = self.session.post(self.api_url, json=test_payload, timeout=10)
            
            if response.status_code == 200:
                logger.info("✓ Successfully connected to API")
            elif response.status_code == 401:
                logger.warning("API key authentication failed - check your credentials")
            elif response.status_code == 404:
                logger.warning("API endpoint not found - check the URL")
            else:
                logger.warning(f"API connection test returned status: {response.status_code}")
                logger.warning(f"Response: {response.text}")
                
        except requests.exceptions.SSLError as e:
            logger.error(f"SSL Error: {str(e)}")
            logger.info("Continuing with offline analysis only...")
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Connection Error: {str(e)}")
            logger.info("Continuing with offline analysis only...")
        except Exception as e:
            logger.error(f"API connection test failed: {str(e)}")
            logger.info("Continuing with offline analysis only...")
    
    def _build_error_patterns(self):
        """Build error patterns from knowledge base"""
        patterns = {}
        for entry in self.knowledge_base:
            error_text = str(entry.get('error', '')).lower()
            solution = entry.get('solution', '')
            if error_text and solution:
                key_words = self._extract_keywords(error_text)
                pattern_key = ' '.join(sorted(key_words))
                if pattern_key:
                    patterns[pattern_key] = {
                        'original_error': entry.get('error'),
                        'solution': solution,
                        'jobname': entry.get('jobname', ''),
                        'keywords': key_words
                    }
        return patterns
    
    def _extract_keywords(self, text):
        """Extract meaningful keywords from error text"""
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'was', 'are', 'were'}
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        return keywords[:5]
    
    def analyze_log_content(self, log_data):
        """Analyze log content and find errors with solutions"""
        results = []
        
        for log_entry in log_data:
            content = log_entry.get('content', '')
            lines = log_entry.get('lines', [])
            filename = log_entry.get('filename', 'unknown')
            
            # Find error lines
            error_lines = self._find_error_lines(lines)
            
            # Analyze each error
            for line_num, error_line in error_lines:
                # First try local matching
                local_analysis = self._analyze_error_line_local(error_line, line_num, filename)
                
                # Enhance with AI if API is available
                ai_enhanced = self._enhance_with_ai(error_line, local_analysis)
                
                results.append(ai_enhanced if ai_enhanced else local_analysis)
        
        return results
    
    def _find_error_lines(self, lines):
        """Find lines containing errors"""
        error_lines = []
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in ERROR_KEYWORDS):
                error_lines.append((i + 1, line))
        return error_lines
    
    def _analyze_error_line_local(self, error_line, line_num, filename):
        """Analyze error line using local knowledge base"""
        error_keywords = self._extract_keywords(error_line)
        best_match = self._find_best_solution_match(error_keywords, error_line)
        
        if best_match:
            return {
                'filename': filename,
                'line_number': line_num,
                'error_line': error_line.strip(),
                'error_keywords': error_keywords,
                'matched_solution': best_match['solution'],
                'original_error': best_match['original_error'],
                'confidence': best_match['confidence'],
                'jobname_reference': best_match.get('jobname', ''),
                'analysis_method': 'local_knowledge_base',
                'analysis_time': datetime.now().isoformat()
            }
        
        return {
            'filename': filename,
            'line_number': line_num,
            'error_line': error_line.strip(),
            'error_keywords': error_keywords,
            'matched_solution': 'No local solution found. Checking with AI...',
            'confidence': 0.1,
            'analysis_method': 'local_fallback',
            'analysis_time': datetime.now().isoformat()
        }
    
    def _enhance_with_ai(self, error_line, local_analysis):
        """Enhance analysis using API with SSL bypass"""
        try:
            # Create context from knowledge base
            knowledge_context = self._build_knowledge_context()
            
            prompt = f"""
You are a log analysis expert. Based on the following knowledge base and error line, provide a specific solution.

KNOWLEDGE BASE EXAMPLES:
{knowledge_context}

ERROR TO ANALYZE:
{error_line}

Please provide:
1. Root cause analysis
2. Specific solution steps
3. Prevention recommendations

Keep the response concise and actionable.
"""

            payload = {
                "model": AI_MODEL,
                "messages": [
                    {"role": "system", "content": "You are an expert log analyzer and troubleshooter."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": MAX_TOKENS,
                "temperature": TEMPERATURE
            }
            
            response = self.session.post(self.api_url, json=payload, timeout=30)
            
            if response.status_code == 200:
                ai_response = response.json()
                ai_solution = ai_response['choices'][0]['message']['content']
                
                # Enhance local analysis with AI insights
                enhanced_analysis = local_analysis.copy()
                enhanced_analysis['ai_solution'] = ai_solution
                enhanced_analysis['matched_solution'] = f"{local_analysis['matched_solution']}\n\nAI ENHANCED SOLUTION:\n{ai_solution}"
                enhanced_analysis['confidence'] = min(local_analysis['confidence'] + 0.3, 1.0)
                enhanced_analysis['analysis_method'] = 'ai_enhanced'
                
                logger.info("✓ Successfully enhanced solution with AI")
                return enhanced_analysis
            else:
                logger.warning(f"AI API returned status {response.status_code}: {response.text}")
            
        except Exception as e:
            logger.error(f"AI enhancement failed: {str(e)}")
        
        return local_analysis
    
    def _build_knowledge_context(self):
        """Build context string from knowledge base for AI prompt"""
        context_examples = []
        for i, entry in enumerate(self.knowledge_base[:5]):  # Limit to 5 examples
            context_examples.append(f"Example {i+1}:")
            context_examples.append(f"Error: {entry.get('error', '')}")
            context_examples.append(f"Solution: {entry.get('solution', '')}")
            context_examples.append("")
        
        return "\n".join(context_examples)
    
    def _find_best_solution_match(self, error_keywords, full_error_line):
        """Find best matching solution from knowledge base"""
        best_match = None
        highest_score = 0
        
        for pattern_key, pattern_data in self.error_patterns.items():
            pattern_keywords = set(pattern_data['keywords'])
            error_keywords_set = set(error_keywords)
            
            if len(pattern_keywords.union(error_keywords_set)) > 0:
                overlap = len(pattern_keywords.intersection(error_keywords_set))
                total_keywords = len(pattern_keywords.union(error_keywords_set))
                keyword_score = overlap / total_keywords
            else:
                keyword_score = 0
            
            # Text similarity score
            text_score = 0
            original_error_lower = pattern_data['original_error'].lower()
            full_error_lower = full_error_line.lower()
            
            common_phrases = self._find_common_phrases(original_error_lower, full_error_lower)
            if common_phrases:
                text_score = 0.3
            
            combined_score = (keyword_score * 0.7) + (text_score * 0.3)
            
            if combined_score > highest_score and combined_score > 0.2:
                highest_score = combined_score
                best_match = {
                    'solution': pattern_data['solution'],
                    'original_error': pattern_data['original_error'],
                    'jobname': pattern_data.get('jobname', ''),
                    'confidence': round(combined_score, 2)
                }
        
        return best_match
    
    def _find_common_phrases(self, text1, text2):
        """Find common phrases between two texts"""
        words1 = text1.split()
        words2 = text2.split()
        
        common = []
        for word in words1:
            if len(word) > 3 and word in words2:
                common.append(word)
        
        return common
    
    def generate_summary_report(self, analysis_results):
        """Generate summary report of analysis"""
        if not analysis_results:
            return "No errors found in the uploaded files."
        
        total_errors = len(analysis_results)
        ai_enhanced = len([r for r in analysis_results if r.get('analysis_method') == 'ai_enhanced'])
        local_solved = len([r for r in analysis_results if r.get('confidence', 0) > 0.5])
        
        report = f"""
LOG ANALYSIS SUMMARY (UBS AI Enhanced)
======================================
Total Errors Found: {total_errors}
AI Enhanced Solutions: {ai_enhanced}
Local Knowledge Solutions: {local_solved}
Success Rate: {((ai_enhanced + local_solved)/total_errors)*100:.1f}%

API Endpoint: {self.api_url}
Model Used: {AI_MODEL}
SSL Verification: Disabled (for corporate networks)

FILES ANALYZED:
{', '.join(set([r['filename'] for r in analysis_results]))}

ERROR DISTRIBUTION:
"""
        
        # Group errors by type
        error_groups = {}
        for result in analysis_results:
            key = ' '.join(result.get('error_keywords', [])[:2])
            if key in error_groups:
                error_groups[key].append(result)
            else:
                error_groups[key] = [result]
        
        for error_type, errors in list(error_groups.items())[:5]:
            report += f"\n• {error_type}: {len(errors)} occurrences"
        
        return report

# Test the analyzer
if __name__ == "__main__":
    print("Testing UBS AI Analyzer with SSL bypass...")
    sample_kb = [
        {
            'jobname': 'TestJob1',
            'error': 'Connection timeout to database',
            'solution': 'Check network connectivity and increase timeout values'
        }
    ]
    
    analyzer = AIAnalyzer(sample_kb)
    print("✓ AI Analyzer initialized with UBS API integration")



# Core dependencies
pandas>=1.5.0
openpyxl>=3.0.0
python-dotenv>=1.0.0

# API integration with SSL bypass
requests>=2.28.0
urllib3>=2.0.4

# Built-in Python modules (no need to install)
# tkinter, pathlib, logging, zipfile, json, re, datetime, threading
