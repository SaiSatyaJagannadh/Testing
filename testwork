import re
import json
import requests
import urllib3
from datetime import datetime
from config import OPENAI_API_KEY, OPENAI_BASE_URL, AI_MODEL, MAX_TOKENS, TEMPERATURE, ERROR_KEYWORDS
import logging
import time

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIAnalyzer:
    def __init__(self, knowledge_base):
        self.knowledge_base = knowledge_base
        self.error_patterns = self._build_error_patterns()
        self.api_headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # Fix the URL - remove duplicate /chat/completions
        base_url = OPENAI_BASE_URL.rstrip('/')
        if base_url.endswith('/chat/completions'):
            self.api_url = base_url
        else:
            self.api_url = f"{base_url}/chat/completions"
        
        # Configure session with SSL bypass and longer timeout
        self.session = requests.Session()
        self.session.headers.update(self.api_headers)
        self.session.verify = False  # Disable SSL verification
        
        # Pre-build AI context from knowledge base (do this once at startup)
        self.knowledge_context = self._build_knowledge_context()
        self.api_available = False
        
        # Test API connection and cache availability
        self._test_api_connection()
        
        logger.info(f"AI Analyzer initialized with {len(self.knowledge_base)} knowledge entries")
        logger.info(f"Built {len(self.error_patterns)} error patterns for quick matching")
        logger.info(f"API Status: {'Available' if self.api_available else 'Offline - using local knowledge only'}")
    
    def _test_api_connection(self):
        """Test connection to API with SSL bypass"""
        try:
            test_payload = {
                "model": AI_MODEL,
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 5
            }
            
            logger.info(f"Testing API connection to: {self.api_url}")
            
            # Use shorter timeout for connection test
            response = self.session.post(self.api_url, json=test_payload, timeout=5)
            
            if response.status_code == 200:
                logger.info("âœ“ API connection successful - AI enhancement available")
                self.api_available = True
            elif response.status_code == 401:
                logger.warning("âš  API key authentication failed - using local knowledge only")
                self.api_available = False
            elif response.status_code == 404:
                logger.warning("âš  API endpoint not found - using local knowledge only")
                self.api_available = False
            else:
                logger.warning(f"âš  API returned status {response.status_code} - using local knowledge only")
                self.api_available = False
                
        except requests.exceptions.Timeout:
            logger.warning("âš  API connection timeout - using local knowledge only")
            self.api_available = False
        except requests.exceptions.SSLError:
            logger.warning("âš  SSL error - using local knowledge only")
            self.api_available = False
        except requests.exceptions.ConnectionError:
            logger.warning("âš  Connection error - using local knowledge only")
            self.api_available = False
        except Exception as e:
            logger.warning(f"âš  API test failed: {str(e)} - using local knowledge only")
            self.api_available = False
    
    def _build_error_patterns(self):
        """Build error patterns from knowledge base (DONE AT STARTUP)"""
        logger.info("Building error patterns from knowledge base...")
        patterns = {}
        for entry in self.knowledge_base:
            error_text = str(entry.get('error', '')).lower()
            solution = entry.get('solution', '')
            if error_text and solution:
                key_words = self._extract_keywords(error_text)
                pattern_key = ' '.join(sorted(key_words))
                if pattern_key:
                    patterns[pattern_key] = {
                        'original_error': entry.get('error'),
                        'solution': solution,
                        'jobname': entry.get('jobname', ''),
                        'keywords': key_words
                    }
        logger.info(f"âœ“ Built {len(patterns)} error patterns for fast matching")
        return patterns
    
    def _build_knowledge_context(self):
        """Build context string from knowledge base (DONE AT STARTUP)"""
        logger.info("Preparing AI context from knowledge base...")
        context_examples = []
        for i, entry in enumerate(self.knowledge_base[:10]):  # Use more examples for better context
            context_examples.append(f"Example {i+1}:")
            context_examples.append(f"Job: {entry.get('jobname', 'N/A')}")
            context_examples.append(f"Error: {entry.get('error', '')}")
            context_examples.append(f"Solution: {entry.get('solution', '')}")
            context_examples.append("")
        
        context = "\n".join(context_examples)
        logger.info(f"âœ“ Prepared AI context with {len(self.knowledge_base)} examples")
        return context
    
    def _extract_keywords(self, text):
        """Extract meaningful keywords from error text"""
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'was', 'are', 'were'}
        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        return keywords[:5]
    
    def analyze_log_content(self, log_data):
        """Analyze log content and find errors with solutions (FAST - uses pre-built patterns)"""
        results = []
        
        logger.info(f"Analyzing {len(log_data)} log files...")
        
        for log_entry in log_data:
            content = log_entry.get('content', '')
            lines = log_entry.get('lines', [])
            filename = log_entry.get('filename', 'unknown')
            
            # Find error lines (FAST)
            error_lines = self._find_error_lines(lines)
            logger.info(f"Found {len(error_lines)} errors in {filename}")
            
            # Analyze each error (uses pre-built patterns)
            for line_num, error_line in error_lines:
                # Local analysis (INSTANT - uses pre-built patterns)
                local_analysis = self._analyze_error_line_local(error_line, line_num, filename)
                
                # AI enhancement with better timeout handling
                if self.api_available and local_analysis.get('confidence', 0) < 0.7:
                    ai_enhanced = self._enhance_with_ai_safe(error_line, local_analysis)
                    results.append(ai_enhanced if ai_enhanced else local_analysis)
                else:
                    results.append(local_analysis)
        
        return results
    
    def _find_error_lines(self, lines):
        """Find lines containing errors (FAST)"""
        error_lines = []
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in ERROR_KEYWORDS):
                error_lines.append((i + 1, line))
        return error_lines
    
    def _analyze_error_line_local(self, error_line, line_num, filename):
        """Analyze error line using pre-built patterns (INSTANT)"""
        error_keywords = self._extract_keywords(error_line)
        best_match = self._find_best_solution_match(error_keywords, error_line)
        
        if best_match and best_match['confidence'] > 0.3:
            return {
                'filename': filename,
                'line_number': line_num,
                'error_line': error_line.strip(),
                'error_keywords': error_keywords,
                'matched_solution': best_match['solution'],
                'original_error': best_match['original_error'],
                'confidence': best_match['confidence'],
                'jobname_reference': best_match.get('jobname', ''),
                'analysis_method': 'local_knowledge_base',
                'analysis_time': datetime.now().isoformat()
            }
        
        return {
            'filename': filename,
            'line_number': line_num,
            'error_line': error_line.strip(),
            'error_keywords': error_keywords,
            'matched_solution': 'Low confidence local match. May need AI enhancement.',
            'confidence': 0.1,
            'analysis_method': 'local_fallback',
            'analysis_time': datetime.now().isoformat()
        }
    
    def _enhance_with_ai_safe(self, error_line, local_analysis):
        """Enhance analysis using API with proper timeout and retry handling"""
        if not self.api_available:
            return local_analysis
        
        max_retries = 2
        retry_delay = 1  # seconds
        
        for attempt in range(max_retries):
            try:
                prompt = f"""
Based on the knowledge base examples, analyze this error and provide a solution:

KNOWLEDGE BASE:
{self.knowledge_context}

ERROR TO ANALYZE:
{error_line}

Provide a clear, actionable solution with specific steps.
"""

                payload = {
                    "model": AI_MODEL,
                    "messages": [
                        {"role": "system", "content": "You are an expert log analyzer. Provide concise, actionable solutions."},
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE
                }
                
                # Use longer timeout for actual AI calls
                response = self.session.post(self.api_url, json=payload, timeout=45)
                
                if response.status_code == 200:
                    ai_response = response.json()
                    ai_solution = ai_response['choices'][0]['message']['content']
                    
                    # Enhance local analysis with AI insights
                    enhanced_analysis = local_analysis.copy()
                    enhanced_analysis['ai_solution'] = ai_solution
                    enhanced_analysis['matched_solution'] = f"{local_analysis['matched_solution']}\n\nðŸ¤– AI ENHANCED SOLUTION:\n{ai_solution}"
                    enhanced_analysis['confidence'] = min(local_analysis['confidence'] + 0.4, 1.0)
                    enhanced_analysis['analysis_method'] = 'ai_enhanced'
                    
                    logger.info("âœ“ Successfully enhanced solution with AI")
                    return enhanced_analysis
                else:
                    logger.warning(f"AI API returned status {response.status_code} on attempt {attempt + 1}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"AI API timeout on attempt {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
            except Exception as e:
                logger.warning(f"AI enhancement failed on attempt {attempt + 1}: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
        
        # If all attempts failed, return local analysis
        logger.warning("AI enhancement failed after all retries - using local solution")
        return local_analysis
    
    def _find_best_solution_match(self, error_keywords, full_error_line):
        """Find best matching solution from pre-built patterns (FAST)"""
        best_match = None
        highest_score = 0
        
        for pattern_key, pattern_data in self.error_patterns.items():
            pattern_keywords = set(pattern_data['keywords'])
            error_keywords_set = set(error_keywords)
            
            if len(pattern_keywords.union(error_keywords_set)) > 0:
                overlap = len(pattern_keywords.intersection(error_keywords_set))
                total_keywords = len(pattern_keywords.union(error_keywords_set))
                keyword_score = overlap / total_keywords
            else:
                keyword_score = 0
            
            # Text similarity score
            text_score = 0
            original_error_lower = pattern_data['original_error'].lower()
            full_error_lower = full_error_line.lower()
            
            common_phrases = self._find_common_phrases(original_error_lower, full_error_lower)
            if common_phrases:
                text_score = len(common_phrases) * 0.1  # Better scoring for multiple matches
            
            combined_score = (keyword_score * 0.7) + (text_score * 0.3)
            
            if combined_score > highest_score and combined_score > 0.2:
                highest_score = combined_score
                best_match = {
                    'solution': pattern_data['solution'],
                    'original_error': pattern_data['original_error'],
                    'jobname': pattern_data.get('jobname', ''),
                    'confidence': round(combined_score, 2)
                }
        
        return best_match
    
    def _find_common_phrases(self, text1, text2):
        """Find common phrases between two texts"""
        words1 = text1.split()
        words2 = text2.split()
        
        common = []
        for word in words1:
            if len(word) > 3 and word in words2:
                common.append(word)
        
        return common
    
    def generate_summary_report(self, analysis_results):
        """Generate summary report of analysis"""
        if not analysis_results:
            return "No errors found in the uploaded files."
        
        total_errors = len(analysis_results)
        ai_enhanced = len([r for r in analysis_results if r.get('analysis_method') == 'ai_enhanced'])
        local_solved = len([r for r in analysis_results if r.get('confidence', 0) > 0.5])
        
        report = f"""
LOG ANALYSIS SUMMARY (UBS AI Enhanced)
======================================
Total Errors Found: {total_errors}
AI Enhanced Solutions: {ai_enhanced}
Local Knowledge Solutions: {local_solved}
Success Rate: {((ai_enhanced + local_solved)/total_errors)*100:.1f}%

Knowledge Base: {len(self.knowledge_base)} pre-loaded solutions
Error Patterns: {len(self.error_patterns)} built for fast matching
API Status: {'Available' if self.api_available else 'Offline'}
Model Used: {AI_MODEL}

FILES ANALYZED:
{', '.join(set([r['filename'] for r in analysis_results]))}

ERROR DISTRIBUTION:
"""
        
        # Group errors by type
        error_groups = {}
        for result in analysis_results:
            key = ' '.join(result.get('error_keywords', [])[:2])
            if key in error_groups:
                error_groups[key].append(result)
            else:
                error_groups[key] = [result]
        
        for error_type, errors in list(error_groups.items())[:5]:
            report += f"\nâ€¢ {error_type}: {len(errors)} occurrences"
        
        return report

# Test the analyzer
if __name__ == "__main__":
    print("Testing UBS AI Analyzer with optimized startup...")
    sample_kb = [
        {
            'jobname': 'TestJob1',
            'error': 'Connection timeout to database',
            'solution': 'Check network connectivity and increase timeout values'
        }
    ]
    
    analyzer = AIAnalyzer(sample_kb)
    print("âœ“ AI Analyzer ready - knowledge base pre-loaded for fast analysis")
